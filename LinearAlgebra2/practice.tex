\documentclass[11pt,a4paper]{article}
\usepackage{amssymb,amsfonts,amsmath,calc,tikz,geometry}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\usepackage{amsthm}
\hypersetup{
    colorlinks=false, %set true if you want colored links
    linktoc=all,   %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}
%\setlength{\textwidth}{450pt}
%\setlength{\oddsidemargin}{7pt}
\geometry{margin=1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\idealin}{\triangleleft}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\Out}{Out}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\std}{std}
\newcommand{\Id}{\text{Id}}
\newcommand{\op}{\mathrm{op}}
\newcommand{\Ker}{\text{Ker}}
\renewcommand{\Im}{\text{Im}}
\newcommand{\Sp}{\text{Sp}}
\newcommand{\tr}{\text{tr}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Omicron}{O}
\newcommand{\bigslant}[2]
{{\raisebox{.2em}{$#1$}\left/\raisebox{-.2em}{$#2$}\right.}}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\textbf{Practice}}
\author{Yeheli Fomberg}
\date{326269651}
\begin{document}
	\maketitle
	\newpage
	
	\section{In the following sections show that $V = U \oplus W$ and find
	the projection on $U$ parallel to $W$}
	\subsection{$V = \R[x]$ with 
	\[ W = \text{Sp}\{x^2+x+1\},\quad U = \{p(x)\in V \colon p(0) = 0\} \]
	}
	
	First we will show that $U + W = V$. Let $p\in V$ be a general polynomial:
	\[ p(x) = a_nx^n + \cdots + a_0 \in V\]
	Now choose $w = a_0(x^2+x+1) \in W$ and $u = (p-w) \in U$. We know that
	$(p-w)\in U$ because: 
	\[ (p-w)(0) = p(0) - w(0) = a_0 - a_0 = 0 \]
	Now we see that: 
	\[ u + w = (p-w) + w = p \] 
	That proves that $U + W = P$. Now we will show that $U\bigcap W = \{0\}$
	which will prove that $U\oplus W = V$, as we have shown in the lecture.
	\begin{align*}
		U\cap W &= \{p(x)\in V \colon p(x)\in W \land p(0) = 0\} \\
		&= \{ax^2+ax+a \colon a\in\R \land p(0) = 0\} \\
		&= \{ax^2+ax+a \colon a\in\R \land a = 0\} \\
		&= \{0\}
	\end{align*}
	Now we will find the projection on $U$ parallel to $W$. We have shown that
	the only way to get any specific $p\in V$ is by adding the specific:
	\[
		u_p + w_p = (p-a_0(x^2+x+1)) + a_0(x^2+x+1)
	\]
	So the parallel projection will be $P \colon V\to V$:
	\begin{align*}
		P(p(x)) &= P(a_nx^n + \cdots + a_0) = u_p = (p-a_0(x^2+x+1)) \\
		&= a_nx^n + \cdots + a_3x^3 + (a_2-a_0)x^2 + (a_1-a_0)x
	\end{align*}
	
	\newpage
	
	\subsection{$V = \R^4$ with 
	\[ W = \text{Sp}\{e_1+e_4,e_2+e_4\},\quad U = \{e_1,e_2+e_3\} \] 
	where $E = (e_1,...,e_4)$ is the standard basis.}
	Consider the following matrix with the vectors from $U$ and $W$:
	\[ \begin{pmatrix}
	\text{---} & e_1+e_4 & \text{---}\\
	\text{---} & e_2+e_4 & \text{---}\\
	\text{---} & e_1 & \text{---}\\
	\text{---} & e_2+e_3 & \text{---}\\
	\end{pmatrix} \]
	By applying elementary row operations we get:
	\[ \begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 1\\
	\end{pmatrix} \]
	Which means as we know from linear algebra $1$ that $U + W = V$ and from
	Grassman's identity(?) we know that:
	\begin{align*}
		\underbrace{\dim(W+U)}_4 &= \underbrace{\dim(W)}_2
		+ \underbrace{\dim(U)}_2 - \dim(U\cap W) \\
		&\Rightarrow \dim(U\cap W) = 0 \\
		&\Rightarrow U\cap W = \{0\}
	\end{align*}
	Which implies that $U\oplus W = V$. Now we will find the projection on 
	$U$ parallel to $W$. For this we will need to find the unique decomposition
	of any $v\in V$ to vectors $u\in U$ and $w\in W$. Where for 
	$a,b,c,d,x_1,x_2,x_3,x_4\in\F$:
	\begin{align*}
		w &= a(e_1+e_4) + b(e_2+e_4) = 
		\begin{pmatrix}
	a\\
	b\\
	0\\
	a+b\\
		\end{pmatrix} \\
		u &= c(e_1) + d(e_2+e_3) = 
		\begin{pmatrix}
	c\\
	d\\
	d\\
	0\\
		\end{pmatrix} \\
	\end{align*}
	\[
		u+w = 
		\begin{pmatrix}
	\vert & \vert & \vert & \vert\\
	e_1+e_4 & e_2+e_4 & e_1 & e_2+e_3\\
	\vert & \vert & \vert & \vert\\
		\end{pmatrix}
		\begin{pmatrix}
	a\\
	b\\
	c\\
	d\\
		\end{pmatrix} =
		\begin{pmatrix}
	a+c\\
	b+d\\
	d\\
	a+b\\
		\end{pmatrix}
		= v =
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}	
	\]
	So we get that $d=x_3$
	\[
		\begin{pmatrix}
	a+c\\
	b+x_3\\
	x_3\\
	a+b\\
		\end{pmatrix}
		=
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}	
	\]
	Now $b=x_2-x_3$
	\[
		\begin{pmatrix}
	a+c\\
	x_2\\
	x_3\\
	a+x_2-x_3\\
		\end{pmatrix}
		=
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}	
	\]
	So $a = x_4-x_2+x_3$ and we get:
	\[
		\begin{pmatrix}
	x_4-x_2+x_3+c\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}
		=
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}	
	\]
	So $c = x_1 - x_4 + x_2 - x_3$. Finally we get that for any $v\in V$ such
	that:
	\[
		v =
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}	
	\]
	We get:
	\begin{align*}
		v &= w + u = a(e_1+e_4) + b(e_2+e_4) + c(e_1) + d(e_2+e_3) \\
		  &= (x_4-x_2+x_3)(e_1+e_4) + (x_2-x_3)(e_2+e_4) + 
		     (x_1 - x_4 + x_2 - x_3)(e_1) + (x_3)(e_2+e_3)
	\end{align*}
	Which means the projection on $U$ parallel to $W$ is $P\colon V\to V$
	\[
		\forall\underbrace{
		\begin{pmatrix}
	x_1\\
	x_2\\
	x_3\\
	x_4\\
		\end{pmatrix}}_v
		\in V \colon P(v) = (x_1 - x_4 + x_2 - x_3)(e_1) + (x_3)(e_2+e_3) = 
		\begin{pmatrix}
	x_1 - x_4 + x_2 - x_3\\
	x_3\\
	x_3\\
	0\\
		\end{pmatrix} = u
	\]

	
	\newpage
	
	\section{Prove/Disprove}
	\subsection{The sum of projections is a projection}
	This is false. Let $P_1 = P_2 = \mathrm{Id}_n$ be our projections from $\R^n$ to
	$\R^n$. It is clear these are projections since:
	\[ \mathrm{Id}_{n}^{2} = \mathrm{Id}_n \]
	But the transformation $P = P_1 + P_2$ is not a projection since:
	\[
		P^2 = (P_1 + P_2)^2 = (2\mathrm{Id}_n)^2 = 4\mathrm{Id}_n \neq 2\mathrm{Id}_n = P
	\]
	
	\subsection{The composition of projections is a projection}
	This claim is false. Consider the following projections over $\R^2$:
	\[
		P_1(x,y) = (x+y,0) \quad and \quad P_2(x,y) = (x,x)
	\]
	It's easy to verify that these are indeed projections:
	\begin{align*}
		P_1^2(x,y) &= P_1(x+y,0) = (x+y,0) = P_1(x,x) \\
		P_2^2(x,y) &= P_2(x,x) = (x,x) = P_2(x,y)
	\end{align*}
	Yet if we consider the vector $(2,1)$ we get:
	\begin{align*}
		(P_1\circ P_2)(2,1) &= P_1(2,2) = (4,0) \\
		(P_1\circ P_2)^2(2,1) &= (P_1\circ P_2)(4,0) = P_1(4,4) = (8,0)
	\end{align*}
	So:
	\[
		(P_1\circ P_2) \neq (P_1\circ P_2)^2
	\]
	Which means it's not a projection.
	
	
	
	\newpage
	
	\section{Let $V$ be a finite-dimensional vector space, and let
	$P_1,...,P_n\in \text{End}(V)$ be parallel projections. Denote $\forall i\colon R_i = \text{Im}P_i$}
	\subsection{Show that $\text{tr}P_i=\dim R_i$}
	Since $P_i$ is a parallel projection we know that 
	$V = \Im P_i \oplus \Ker P_i$
	Which means that $\Im P_i\cap \Ker P_i = \{0\}$. We know by a theorem
	we learned in class that exist:
	\[ B_r = \{b_1,...,b_k\} \]
	a basis for $\Im P_i = R_i$. And:
	\[ B_k = \{r_{b+1},...,b_n\} \]
	a basis for $\Ker P_i$ such that the ordered union:
	\[ B = B_r \cup B_k = \{b_1,...,b_k,b_{k+1},...,b_n\} \]
	forms a basis for $V$. That means that the matrix representation of $P_i$
	by the basis $B$ is:
	\[ 
	\begin{pmatrix}
	\vert & \vert & \vert\\
	[P_i(b_1)]_B & ... & [P_i(b_n)]_B\\
	\vert & \vert & \vert\\
	\end{pmatrix}_{n\times n}
	= 
	\begin{pmatrix}
	I_k & 0\\
	0 & 0\\
	\end{pmatrix}_{n\times n}
	\]
	So $\tr ([P_i]_B) = k$. And since the trace of a transformation is just
	the trace of its representing matrix, as shown to be a well defined trait
	of transformations  in linear algebra $1$ we conclude that:
	\[
		\tr (P_i) = \tr ([P_i]_B) = k = \dim\Im P_i = \dim R_i
	\]

	\newpage
	
	\subsection{Let $P_1+\cdots+P_n = \text{Id}$, show that 
	$V = \bigoplus R_i$ and infer that $\forall i\neq j \colon P_iP_j = 0$}
	\underline{$V = \bigoplus R_i$} - From $3.1$ we know that:
	\[
		\dim V = \tr (\mathrm{Id}) = \tr (P_1+\cdots+P_n) = 
		\tr (P_1) + \cdots + \tr(P_n) = \dim R_1 + \cdots + \dim R_n
	\]
	Now we will show that $R_1+\cdots R_n = V$. Let $v\in V$:
	\[
		v = \mathrm{Id}(v) = (P_1+\cdots+P_n)(v) = P_1(v)+\cdots+P_n(v)
	\]
	Since $\forall i \colon P_i(v)\in R_i$ we get that for any $v\in V$ exist
	$P_1(v)\in R_1,...,P_n(v)\in R_n$ such that $v = P_1(v)+\cdots+P_n(v)$.
	So now we know that
	\begin{align*}
		V &= R_1 + \cdots + R_n \\
		\dim V &= \dim R_1 + \cdots + \dim R_n
	\end{align*}
	Denote $B_{R_i}$ the ordered basis for $R_i$ for any $i$, we get:
	\begin{align*}
		V &= \Sp \left\{ \bigcup_i{B_{R_i}} \right\}
		&\Rightarrow \dim V \le \left\vert \bigcup_i{B_{R_i}} \right\vert \\
		\dim V &= \sum_i{\vert B_{R_i} \vert} \ge 
		\left \vert \bigcup_i{B_{R_i}} \right \vert
		&\Rightarrow \left \vert \bigcup_i{B_{R_i}} \right \vert \le \dim V \\
		&\Rightarrow \left \vert \bigcup_i{B_{R_i}} \right \vert = \dim V
	\end{align*}
	So from:
	\[
		\Sp \left\{ \bigcup_i{B_{R_i}} \right \}   = V
		\land 
		\left \vert \bigcup_i{B_{R_i}} \right \vert = \dim V
	\]
	We get that the ordered union of the ordered bases $B_{R_i}$ form a basis
	of $V$ which is equivalent as we've shown in class to saying that 
	$V = \bigoplus R_i$ \\
	\underline{$\forall i\neq j \colon P_iP_j = 0$} - Let $i \neq j$. Now
	suppose that $P_iP_j \neq 0$. that means that exists a $0 \neq v\in V$
	such that $P_iP_j(v) \neq 0$, which means that $P_j(v) \notin \Ker P_i$.
	Since $P_i$ is a projection we know that $\Im P_i \oplus \Ker P_i = V$
	which means that $P_j(v) \in R_i$, but also by definition 
	$P_j(v) \in R_j$, so:
	\begin{align*}
		\underbrace{0}_{R_1} + \cdots + \underbrace{P_j(v)}_{R_j} + \cdots + 
		\underbrace{0}_{R_n} = P_j(v) \\
		\underbrace{0}_{R_1} + \cdots + \underbrace{P_j(v)}_{R_i} + \cdots + 
		\underbrace{0}_{R_n} = P_j(v) 
	\end{align*}
	 but that's a contradiction to $V = \bigoplus R_i$. So 
	 $\forall i\neq j \colon P_iP_j = 0$
	
	\newpage
	
	\section{Let $V$ be a vector space, $T,S\in \text{End}(V)$, and let $S$ be 
	diagonalizable. Prove: 
	\[ \text{the eigenspaces of $S$ are $T$-invariant} \iff TS = ST \]}
	{$(\Leftarrow)$} \\
	For any eignenvalue $\lambda$ of $S$:
	\begin{align*}
		\Ker(S-\lambda I) &= \{s \in V \vert S(s) = \lambda s\} \\
		\Rightarrow T(\Ker(S-\lambda I)) &= \{T(s) \vert S(s) = \lambda s\} \\
		&= \{s \in V \vert \exists w\colon T(w) = s \land S(w) = \lambda w\}
	\end{align*}
	Since for $s\in T(\Ker(S-\lambda I))$:
	\[
		S(s) = S(T(w)) \underset{TS=ST}{=} T(S(w)) = T(\lambda w) = 
		\lambda T(w) = \lambda s
	\]
	We get that $T(\Ker(S-\lambda I)) \subseteq \Ker(S-\lambda I)$ which means
	that all the eigenspaces of $S$ are $T$-invariant. \\\\
	{$(\Rightarrow)$} \\
	We know that $S$ is diagnolizable so exist a base to $V$
	\[
	 B = (b_1,\dots,b_n)
	\]
	such that $[S]_B$ is a diagnonal matrix. We will show that for any $b\in B$
	that $TS(b) = ST(b)$. Let $b\in B$ be an eigenvector of an eigenspace
	with eigenvalue $\lambda$:
	\[
		TS(b) = T(\lambda b) = \lambda (T(b))
	\]
	Now since $b\in V_\lambda^S$\footnote{$\lambda$-eigenspace of $S$ under $V$
	not sure if this is the correct notation.} is $T$-invariant by the
	assumption:
	\[
		\lambda (T(b)) = S(T(b)) = ST(b)
	\]
	We have shown that for any vector from the base $B$ of $V$
	\[
		TS(b) = ST(b)
	\]
	Since $B$ spans $V$ and $S$, $T$ are linear, we know that for any $v\in V$
	\[
		TS(v) = ST(v)
	\]
	Which is what we wanted to prove.
	
	\newpage
	
	\section{Let $V$ be a vector space over a field $\F$, with $\dim V = n$. 
	Let $T \colon V \to V$ such that any $(n-1)$-dimentional vector subspace of 
	$V$ is $T$-invariant. Prove that $V$ is a scalar transformation.}
	Let $v_1\in V$ be a vector such that $T(v_1) = v_2$ and $v_2$ isn't a
	scalar multiply of $v_1$. That means they are linearly independent which
	implies we can complete $\{v_1,v_2\}$ to a basis of $V$ as such:
	\[
		B = (v_1,v_2,\dots,v_n)
	\] 
	Since $\Sp\{v_1,v_3,\dots,v_n\}$ is a $n-1$-dimentional subspace of $V$,
	it is $T$-invariant, which means that:
	\[
		T(v_1) = v_2 \in \Sp(v_1,v_3,\dots,v_n)
	\]
	But that's a contradiction since if $v_2$ were in $\Sp(v_1,v_3,\dots,v_n)$
	then $B$ wouldn't be linearly independent even thought it's a basis of $V$.
	That means that for any $v\in V$ then $T(v)$ is a scalar multiple of $v$.
	Now consider the standard basis $E=(e_1,\dots,e_n)$ we know that:
	\begin{align*} 
		T(e_1) &= \lambda_1e_1 \\
		T(e_2) &= \lambda_2e_2 \\
		&\dots \\
		T(e_n) &= \lambda_ne_n \\
	\end{align*}
	We also know that $T(e_1+\cdots+e_n) = \mu\sum_{i=1}^{n}{e_i}$ so:
	\[
		T(e_1+e_2+\cdots+e_n) = T(e_1) + \cdots + T(e_n) = 
		\sum_{i=1}^{n}{\lambda_ie_i} = \sum_{i=1}^{n}{\mu e_i}
	\]
	Since $e_1,\dots,e_n$ are linearly independent that means that:
	\[
		\lambda_1=\lambda_2=\dots=\lambda_n=\mu
	\]
	Finally since $E$ is a basis, for any $v\in V$ we get that $T(v) = \mu v$.
	In other words that $T$ is a scalar operator.
	
	\newpage
	
	\section{Let $T,S,Q\in\text{End}(v)$ such that $T = Q^{-1}SQ$. 
	Show that $U\subseteq V$ is $T$-invariant $\iff Q(U)$ is $S$-invariant}
	\underline{$(\Rightarrow)$} Suppose that $U\subseteq V$ is $T$-invariant.
	That means that:
	\[
		T(U)\subseteq U
	\]
	Now:
	\[
		S(Q(U)) = SQ(U)
	\]
	But we know that $T = Q^{-1}SQ \Rightarrow QT = SQ$ so:
	\[
		S(Q(U)) = QT(U) = Q(T(U))
	\]
	We know that $T(U)\subseteq U$ so:
	\begin{align*}
		S(Q(U)) &= Q(T(U)) \subseteq Q(U) \\
		&\Rightarrow S(Q(U)) \subseteq Q(U)
	\end{align*}
	In other words - $Q(U)$ is $S$-invariant. \\\\
	\underline{$(\Leftarrow)$} Suppose that $Q(U)$ is $S$-invariant:
	\[
		(*) \quad S(Q(U)) \subseteq Q(U)
	\]
	Now:
	\[
		T(U) = Q^{-1}SQ(U) = Q^{-1}(S(Q(U))) \underset{*}{\subseteq} 
		Q^{-1}(Q(U)) = U
	\]
	So:
	\[
		T(U) \subseteq U
	\]
	In other words $U$ is $T$-invariant.
	
	\newpage
	
	\section{The one it won't be fun to typeset.}
	\subsection{Find the Jordan normal form, a jordan basis, and the minimal
	polynomial of the following matrix:
	\[ A = 
\begin{pmatrix}
	-1 & -1 & 0 & 0 \\
	2 & 2 & 0 & 0 \\
	4 & 2 & 2 & 1 \\
	-2 & -1 & -1 & 0 \\
\end{pmatrix}\in M_n(\C)
	\]}
	First we're gonna find the characteristic polynomial of this matrix. We
	notice that the matrix is a blockwise triangular matrix:
	\[ A = 
\begin{pmatrix}
	-1 & -1 & 0 & 0\\
	2 & 2 & 0 & 0\\
	4 & 2 & 2 & 1\\
	-2 & -1 & -1 & 0\\
\end{pmatrix}
		=
\begin{pmatrix}
	B_{2\times 2} & 0\\
	* & C_{2\times 2}
\end{pmatrix}
	\]
	So we can solve it like we did in linear algebra $1$:
	\begin{align*}
		p_A(\lambda) &= p_B(\lambda)p_C(\lambda) = 
		((-1-\lambda)(2-\lambda)+2)((2-\lambda)(0-\lambda)+2) \\
		&= (\lambda^2-\lambda)(\lambda^2-2\lambda+1) = 
		   (\lambda(\lambda-1))(\lambda^2-2\lambda+1) = 
		   \lambda(\lambda-1)^3
	\end{align*}
	But we can also notice that the sum of columns of these blocks is $1$ so
	$1$ is an eigenvalue of both of them, and since the sum of the eigenvalues
	of a matrix is equal to its trace we can find the other eigen value.
	We see that $\lambda = 0$ is an eigenvalue of algebraic multiplicity $1$
	and $\lambda = 1$ is an eigenvalue of algebraic multiplicity $3$ so the\
	Jordan normal form will have a Jordan block $J_1(0)$ and some Jordan blocks
	of total size $3$. Now we will find $\null(A-I)$ to find out how many Jordan
	blocks are there:
	\begin{align*}
			A-I &= 
	\begin{pmatrix}
		-2 & -1 & 0 & 0\\
		2 & 1 & 0 & 0\\
		4 & 2 & 1 & 1\\
		-2 & -1 & -1 & -1\\
	\end{pmatrix} \to
	\begin{pmatrix}
		0 & 0 & 0 & 0\\
		2 & 1 & 0 & 0\\
		2 & 1 & 0 & 0\\
		-2 & -1 & -1 & -1\\
	\end{pmatrix} \to
	\begin{pmatrix}
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		2 & 1 & 0 & 0\\
		0 & 0 & -1 & -1\\
	\end{pmatrix} \\ &\to
	\begin{pmatrix}
		2 & 1 & 0 & 0\\
		0 & 0 & -1 & -1\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
	\end{pmatrix}
	\end{align*}
	So we see that $\null(A-I) = 2$ so there are two Jordan blocks of 
	$\lambda = 1$. That that the Jordan normal form of $A$ must be
	of the form $J_2(1) \oplus J_1(1) \oplus J_1(0)$. So we want to find
	Jordan chains of the form:
	\begin{center}
	\begin{tabular}{c c | c} 
	 $\lambda=1$ &  & $\lambda=2$ \\
	 \hline
	 $v_2$ &  &  \\ 
	 $\downarrow$ &  &  \\
	 $v_1$ & $v_3$ & $v_4$ \\
	\end{tabular}
	\end{center}
	We shall continue with some more calculation to find the generalized
	eigenspaces of $A$.	
	\[
		\ker(A-I) = \ker\left(
		\begin{pmatrix}
		2 & 1 & 0 & 0\\
		0 & 0 & -1 & -1\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		\end{pmatrix}\right) = 
		\Sp
		\left\{ \begin{pmatrix} 1\\ -2\\ 0\\ 0\\ \end{pmatrix},
		   \begin{pmatrix} 0\\ 0\\ 1\\ -1\\ \end{pmatrix} \right\}
	\]
	\begin{align*}
		(A-I)^2 &= 
	\begin{pmatrix}
		-2 & -1 & 0 & 0\\
		2 & 1 & 0 & 0\\
		4 & 2 & 1 & 1\\
		-2 & -1 & -1 & -1\\
	\end{pmatrix} \to
	\begin{pmatrix}
		-2 & -1 & 0 & 0\\
		2 & 1 & 0 & 0\\
		2 & 1 & 0 & 0\\
		0 & 0 & 0 & 0\\
	\end{pmatrix} \to
	\begin{pmatrix}
		2 & 1 & 0 & 0\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
	\end{pmatrix} \\
	\ker(A-I)^2 &= \ker\left(
		\begin{pmatrix}
		2 & 1 & 0 & 0\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		\end{pmatrix}\right) = 
		\Sp
		\left\{ \begin{pmatrix} 1\\ -2\\ 0\\ 0\\ \end{pmatrix},
		   \begin{pmatrix} 0\\ 0\\ 1\\ 0\\ \end{pmatrix},
		   \begin{pmatrix} 0\\ 0\\ 0\\ 1\\ \end{pmatrix} \right\} = 
		   \overset{\sim}{V_1} \\
	\ker(A) &= \ker\left(
		\begin{pmatrix}
			-1 & -1 & 0 & 0\\
			2 & 2 & 0 & 0\\
			4 & 2 & 2 & 1\\
			-2 & -1 & -1 & 0\\
		\end{pmatrix}\right) = \ker\left(
		\begin{pmatrix}
			1 & 1 & 0 & 0\\
			0 & 0 & 0 & 0\\
			2 & 0 & 2 & 1\\
			0 & 1 & -1 & 0\\
		\end{pmatrix}\right) = \ker\left(
		\begin{pmatrix}
			1 & 1 & 0 & 0\\
			0 & 0 & 0 & 0\\
			0 & 0 & 0 & 1\\
			0 & 1 & -1 & 0\\
		\end{pmatrix}\right) \\ &= 
		\Sp\left\{\begin{pmatrix} -1\\ 1\\ 1\\ 0\\ \end{pmatrix}\right\} = 
		\overset{\sim}{V_0}
	\end{align*}
	To test our calculations against the generalized eigenspace decomposition
	theorem we see that indeed:
	\[
	V = \Sp\left\{
		\begin{pmatrix} -1\\ 1\\ 1\\ 0\\ \end{pmatrix}\right\} \oplus
		\Sp\left\{
		\begin{pmatrix} 1\\ -2\\ 0\\ 0\\ \end{pmatrix},
		\begin{pmatrix} 0\\ 0\\ 1\\ 0\\ \end{pmatrix},
		\begin{pmatrix} 0\\ 0\\ 0\\ 1\\ \end{pmatrix}\right\} = 
		\overset{\sim}{V_0} \oplus \overset{\sim}{V_1}
	\]
	To find $v_2$ we would need to find a vector in $\ker(A-I)^2$ that
	is not in $\ker(A-I)$ for example:
	\[
		v_2 = \begin{pmatrix} 0\\ 0\\ 1\\ 1\\ \end{pmatrix}
	\]
	Then:
	\[
		v_1 = (A-I)v_2 = \begin{pmatrix} 0\\ 0\\ 2\\ 0\\ \end{pmatrix}
	\]
	Now fo find $v_3$ we will just find a vector that will complement 
	$\Sp\{v_1,v_2\}$ to $\overset{\sim}{V_1}$ for example:
	\[
		v_3 = \begin{pmatrix} 1\\ -2\\ 0\\ 0\\ \end{pmatrix}
	\]
	And for the last vector we can just choose any vector that is in 
	$\overset{\sim}{V_1}$ for example:
	\[
		v_4 = \begin{pmatrix} -1\\ 1\\ 1\\ 0\\ \end{pmatrix}
	\]
	So we found all of our Jordan chains and also the Jordan basis for $A$:
	\[
		B_J = 
		\{
		\begin{pmatrix} 0\\ 0\\ 2\\ 0\\ \end{pmatrix}
		\begin{pmatrix} 0\\ 0\\ 1\\ 1\\ \end{pmatrix}
		\begin{pmatrix} 1\\ -2\\ 0\\ 0\\ \end{pmatrix}
		\begin{pmatrix} -1\\ 1\\ 1\\ 0\\ \end{pmatrix}
		\}
	\]
	Now we will find the minimal polynomial. To find the minimal polynomial
	we will see that it is excactly the product of the the polynomials
	of the form $p(x)=(x-\lambda)^r$ for each distinct eigenvalue $\lambda$
	of $A$ and $r$ being the size of the longest Jordan chain of its
	respective $\lambda$, since each vector in $V$ can be represented as a 
	linear combination of the Jordan base, and for any polynome that doesn't 
	include one of these multiples of $(x-\lambda)$ we can take the top of
	the chain of this lambda and see that it will not be a root of the supposed
	polynome. Therefore:
	\[
		m_A(x) = (x-1)^2 (x-2)
	\]

	\newpage
	
	\section{The one with the polynomial operator}
	\subsection{Let $T\colon \R_3[x]\to\R_3[x]$ be the operator 
	\[
	T(ax^3+bx^2+cx+d) = 2ax^3+(2b+3c+d)x^2+(2c+3d)x+2d
	\]
	Does exist a basis to $\R_3[x]$ such that: 
	\[ 
	[T^2-4T+4I]_B = 
	\begin{pmatrix}
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 0\\
	0 & 0 & 0 & 0\\
	\end{pmatrix} \]}
	We notice that:
	\[ [T^2-4T+4I]_B = 
	\begin{pmatrix}
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 0\\
	0 & 0 & 0 & 0\\
	\end{pmatrix}    = J_3(0) \oplus J_1(0) \]
	And since we know that the Jordan normal form of a transformation
	is unqiue up to order, it suffices to show that the Jordan normal form
	of $T^2-4T+4I$ is the same or different than $J_3(0) \oplus J_1(0)$.
	Making some calculations we get that represented by the standard
	basis:
	\[
		[T^2-4T+4I]_E = 
		\begin{pmatrix}
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 9\\
		0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0\\
		\end{pmatrix} \underbrace{=}_{\text{denotion}} B 
	\]
	Which means that the characteristic polynomial of it is:
	\[
		p_{B}(x) = 
		\begin{vmatrix}
	-\lambda & 0 & 0 & 0\\
	0 & -\lambda & 0 & 9\\
	0 & 0 & -\lambda & 0\\
	0 & 0 & 0 & -\lambda\\
		\end{vmatrix} = 
		-\lambda(-\lambda^3+9*0) =
		\lambda^4
	\]
	So the only eigenvalue of $T^2-4T+4I$ is $0$, of algebraic multiplicity $4$.
	We know by a theorem we have proved in class that there must be at least:
	\[\dim\ker(T^2-4T+4I) = 3\]
	Jordan blocks in $T^2-4T+4I$'s Jordan normal form. This means that it
	can't have the Jordan normal form of $J_3(0) \oplus J_1(0)$, so we have
	shown that there does not exist a basis $B$ to $V$ such that 
	\[[T^2-4T+4I]_B = 
	\begin{pmatrix}
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 0\\
	0 & 0 & 0 & 0\\
	\end{pmatrix}\]

	\newpage

	\section{The one with the ranks}
	\subsection{Let $A\in M_7(\R)$ such that:
	\[
		rk(A-I)^2 > rk(A-I)^3 = rk(A-I)^4
	\] and $rk(A)=3$. Calculate the Jordan normal form of $A$.}
	We know that $rk(A)=\dim\Im(A) = 3$ and since we also know that:
	\[
		\underbrace{\dim\Im(A)}_3 + \dim\ker(A) = \underbrace{\dim \R^7}_7
	\]
	We know that $\dim\ker(A) = 4$ which tells us that there are $4$ Jordan blocks
	in the Jordan normal form of $A$ with eigenvalue $0$. From similar 
	considerations we also see that:
	\[
		\dim\ker(A-I)^3 = 7 - rk(A-I)^3 = 7 - rk(A-I)^4 = \dim\ker(A-I)^4
	\]
	So we know that there are:
	\[ \dim\ker(A-I)^4 - \dim\ker(A-I)^3 = 0 \]
	Jordan blocks with eigenvalue $1$ of size at least $4$. Also:
	\[
		\dim\ker(A-I)^2 = 7 - rk(A-I)^2 < 7 - rk(A-I)^3 = \dim\ker(A-I)^3
	\]
	So there is at least $1$ Jordan block of size $3$ in the Jordan normal form 
	of $A$. Since as we have shown, there must be $4$ Jordan blocks in the Jordan
	normal form with eigenvalue $0$, and the sum of the order of the Jordan blocks
	must be equal to $7$ the only option for the Jordan normal form of $A$ is:
	\[
		J_3(1) \oplus J_1(0) \oplus J_1(0) \oplus J_1(0) \oplus J_1(0)
	\]  

	\newpage

	\section{The one with the inverses}
	\subsection{Let $\F$ be a field and $0\neq\lambda\in\F$. Find the Jordan 
	normal form of $J_n(\lambda)^{-1}$. No need to explicitly compute the
	inverse.}
	We can write the Jordan block $J_n(\lambda)$ as the sum of a scalar and
	a nilpotent matrix like so:
	\[
		J_n(\lambda) = \lambda I + J_n(0)
	\]
	Now we notice that since $\lambda \neq 0$ we can multiply both sided by 
	$\lambda^{-1} I$:
	\[
		\lambda^{-1} I J_n(\lambda) =\lambda^{-1} I(\lambda I + J_n(0)) = 
		I + \lambda^{-1}J_n(0)
	\]
	And that:
	\[
		(I - \lambda^{-1}J_n(0))(I + \lambda^{-1}J_n(0)) = 
		I - \lambda^{-2}J_n^2(0)
	\]
	Now since:
	\[
		(I + \lambda^{-2}J_n^2(0))(I - \lambda^{-2}J_n^2(0)) = 
		I - \lambda^{-4}J_n^4(0)
	\]
	We can keep going like:
	\[
		(I + \lambda^{-4}J_n^4(0))(I - \lambda^{-4}J_n^4(0)) = 
		I - \lambda^{-8}J_n^8(0)
	\]
	So we see know that:
	\[
		\left(\prod_{i=1}^{k}{(I + \lambda^{-2^k}J_n^{2^k}(0))}\right)
		(I - \lambda^{-1}J_n(0))(\lambda^{-1} I)J_n(\lambda) = I - \lambda^{-2^{k+1}}
		J_n^{2^{k+1}}(0)
	\]
	Since $J_n(0)$ is nilpotent of order $n-1$ we can choose $k\in\N$ such that
	$2^{k+1} > n$ and then:
	\[
		\left(\prod_{i=1}^{k}{(I + \lambda^{-2^k}J_n^{2^k}(0))}\right)
		(I - \lambda^{-1}J_n(0))(\lambda^{-1} I)J_n(\lambda) = I - \lambda^{-2^{k+1}}
		J_n^{2^{k+1}}(0) = I
	\]
	From linear algebra $1$ we know that a if $AB=I$ then $BA=I$ which means
	that we found the inverse of $J_n(\lambda)$:
	\[
		J_n(\lambda)^{-1} = \left(\prod_{i=1}^{k}{(I + \lambda^{-2^k}J_n^{2^k}(0))}
		\right)(I - \lambda^{-1}J_n(0))(\lambda^{-1} I)
	\]

	\newpage

	\subsection{Find a sufficient and necessary condition that a real matrix has to
	meet to be similar to its inverse.}


	\newpage

	\section{The one with the 9s}
	\subsection{Prove that exists a matrix $A\in M_n(\R)$ that satisfies:
	\[
		A^9+A^{99} = \begin{pmatrix}
		2 & 99 & 999\\
		0 & 2 & -9\\
		0 & 0 & 2\\
	\end{pmatrix}
	\] There's no need to find one explicitly.}
	Since the matrix we get by the calculation is of order $3$ we know that $A$
	is also of order $3$. 1


	Consider $A = J_3(\lambda)$, by a theorem we proved
	in class we can see that for the polynome $f(x) = x^9 + x^{99}$:
	\[
		f(A) = 
		\begin{pmatrix}
		f(\lambda) & f'(\lambda) & \frac{1}{2}f''(\lambda)\\
		0 & f(\lambda) & f'(\lambda)\\
		0 & 0 & f(\lambda)\\
		\end{pmatrix}
	\]

	\newpage

	\section{The one with high powers}
	\subsection{Find all the matrices $A\in M_4(\C)$ that satisfy $A^4-2A^2+1=0$
	up to similarity.}

	\newpage

	\section{The one with invariant subspaces}
	\subsection{Compute the invariant subspaces of a jordan block $J_n(\lambda)$.
	Use what we saw in the rehearsal about the invariant subspaces of $J_n(0)$.}

	\newpage

	\subsection{Let $T\in End(V)$ where $V$ is a complex vector space of finite
	dimension. Show that there is a finite amount of $T$-invariant subspaces iff
	$p_T(x) = m_T(X)$}

\newpage
	\section{The one with the Cauchy-Schwartz inequality}
	
	\subsection{Show that for all positive $x_1,\dots,x_n\in\R$:
	\[
	n^2 \le (x_1+\cdots+x_n)\left(\frac{1}{x_1}+\cdots+\frac{1}{x_n}\right)
	\]}
	Let $x_1,\dots,x_n$ be positive real numbers.
	Recall that the Cauchy-Shwartz inequality states that for any $v,u$ in an 
	inner product space, and specifically for $(\R^n,\langle,\rangle_{\std})$
	we get:
	\[
		\vert\langle v,u\rangle\vert^2 \le \langle v,v\rangle\langle u,u\rangle
	\]
	Since $x_1,\dots,x_n$ are positive we can take their roots and then for:
	\[ 
		v=(\sqrt{x_1},\dots,\sqrt{x_n}) \quad\text{and}\quad 
		u=(\frac{1}{\sqrt{x_1}},\dots,\frac{1}{\sqrt{x_n}})
	\]
	We get:
	\[
		\vert\langle v,u\rangle\vert^2 = 
		\vert\langle (\sqrt{x_1},\dots,\sqrt{x_n}),
		(\frac{1}{\sqrt{x_1}},\dots,\frac{1}{\sqrt{x_n}})\rangle\vert^2 =
		\vert n\vert^2 = n^2
	\]
	And:
	\[
		\langle v,v\rangle\langle u,u\rangle = 
		(x_1+\cdots+x_n)(\frac{1}{x_1}+\cdots+\frac{1}{x_n})
	\]
	Now substituting we get:
	\[
		n^2 \le (x_1+\cdots+x_n)\left(\frac{1}{x_1}+\cdots+\frac{1}{x_n}\right)
	\]
	Which is what we wanted to prove.

	\newpage

	\section{The one with the integral}
	Let $V=\R_2[x]$ and let:
	\begin{align*}
		\langle(p(x),q(x))\rangle_1 = 
		\int_{0}^{1}{p(x)q(x)\,dx} \\
		\langle(p(x),q(x))\rangle_2 = 
		\sum_{x\in\{-1,0,1\}}{p(x)q(x)}
	\end{align*}
	Two inner products on $V$, and let:
	\[
		W = \{p(x)\in V \vert p(x) = p(-x)\}
	\]
	
	\subsection{Find a basis for $W$ and complete it to a basis for $V$.}
	We know that $W\neq V$ and $W\neq 0$ so since $x^2,1\in W$ and are linearly
	independant we get that $\dim W=2$ and that
	\[
		B_W = \{x^2, 1\}
	\]
	is a basis for $W$. We can complete it to a basis for $V$ as such:
	\[
		B_V = \{x^2, 1, x\}
	\]
	
	\newpage

	\subsection{Apply the Gram-Schmidt process on $V$ relative to each of the
	inner products, find $W^\bot$ and the orthogonal projection $P_W$ on $W$.}
	According to $\langle ,\rangle_1$ we get:
	\begin{align*}
		u_1' &= v_1 = x^2 \\
		u_2' &= v_2 - \sum_{i=1}^{1}
		{\frac{\langle v_2,u'_i\rangle}{\langle u'_i,u'_i\rangle}u'_i} = 
		v_2 - \frac{\langle v_2,u'_1\rangle}{\langle u'_1,u'_1\rangle} u'_1 = 
		1 - \frac{\langle 1,x^2\rangle}{\langle x^2,x^2\rangle} x^2 = 
		1 - \frac{\frac{1}{3}}{\frac{1}{5}} x^2 = 1 - \frac{5}{3}x^2 \\
		u_3' &= v_3 - \sum_{i=1}^{2}
		{\frac{\langle v_3,u'_i\rangle}{\langle u'_i,u'_i\rangle}u'_i} = 
		x - \frac{\langle x,x^2\rangle}{\langle x^2,x^2\rangle}x^2 - 
		\frac{\langle x,1\rangle}{\langle 1,1\rangle}1 = 
		x - \frac{4}{3}x^2 - \frac{1}{2}
	\end{align*}
	Now to normalize the vectors:
	\begin{align*}
		u_1 &= \frac{u_1'}{\|u_1'\|} = 
		\frac{x^2}{\sqrt{\langle x^2,x^2\rangle}} = 2x^2
		\\
		u_2 &= \frac{u_2'}{\|u_2'\|} = 
		\frac{1-\frac{5}{3}x^2}
		{\sqrt{\langle 1-\frac{5}{3}x^2,1-\frac{5}{3}x^2\rangle}} = 
		\frac{3}{2}-\frac{5}{2}x^2
		\\
		u_3 &= \frac{u_3'}{\|u_3'\|} = 
		\frac{x - \frac{4}{3}x^2 - \frac{1}{2}}
		{\sqrt{\langle x - \frac{4}{3}x^2 - \frac{1}{2},x - \frac{4}{3}x^2 - 
		\frac{1}{2}\rangle}} = 
		\frac{30}{\sqrt{195}}x-\frac{40}{\sqrt{195}}x^2-\frac{15}{\sqrt{195}}
	\end{align*}
	Notice that when we applied Gram-Schmidt we first found an orthonormal basis
	for $W$, and since we know that $V = W \oplus W^\bot$ we get:
	\[ W^\bot = \Sp\{u_3\} = \Sp
	\left\{\frac{30}{\sqrt{195}}x-\frac{40}{\sqrt{195}}
	x^2-\frac{15}{\sqrt{195}} \right\} \]
	And as we know from the lectures for all $v\in V = \R_2[x]$ we get:
	\begin{align*}
		P_W(v) = \sum_{i=1}^{2}{\langle v(x),u_i \rangle u_i} &= 
		\langle v(x),2x^2 \rangle 2x^2 + \langle v(x),\frac{3}{2} - 
		\frac{5}{2}x^2 \rangle \left(\frac{3}{2}-\frac{5}{2}x^2\right) \\ &= 
		\left(\int_{0}^{1}{v(x) 2x^2\,dx}\right) 2x^2 + 
		\left(\int_{0}^{1}{v(x)\left(\frac{3}{2} - \frac{5}{2}x^2\right)\,dx}
		\right) \left(\frac{3}{2}-\frac{5}{2}x^2\right)
	\end{align*}
	
	\newpage
	
	According to $\langle ,\rangle_2$ we get:
	\begin{align*}
		u_1' &= v_1 = x^2 \\
		u_2' &= v_2 - \sum_{i=1}^{1}
		{\frac{\langle v_2,u'_i\rangle}{\langle u'_i,u'_i\rangle}u'_i} = 
		v_2 - \frac{\langle v_2,u'_1\rangle}{\langle u'_1,u'_1\rangle} u'_1 = 
		1 - \frac{\langle 1,x^2\rangle}{\langle x^2,x^2\rangle} x^2 = 
		1 - \frac{2}{2} x^2 = 1 - x^2 \\
		u_3' &= v_3 - \sum_{i=1}^{2}
		{\frac{\langle v_3,u'_i\rangle}{\langle u'_i,u'_i\rangle}u'_i} = 
		x - \frac{\langle x,x^2\rangle}{\langle x^2,x^2\rangle}x^2 - 
		\frac{\langle x,1\rangle}{\langle 1,1\rangle}1 = 
		x
	\end{align*}
	Now to normalize the vectors:
	\begin{align*}
		u_1 &= \frac{u_1'}{\|u_1'\|} = 
		\frac{x^2}{\sqrt{\langle x^2,x^2\rangle}} = 
		\frac{x^2}{\sqrt{2}}
		\\
		u_2 &= \frac{u_2'}{\|u_2'\|} = 
		\frac{1 - x^2}{\sqrt{\langle 1 - x^2,1 - x^2\rangle}} = 
		1-x^2
		\\
		u_3 &= \frac{u_3'}{\|u_3'\|} = 
		\frac{x}{\sqrt{\langle x, x\rangle}} = 
		\frac{x}{\sqrt{2}}
	\end{align*}
	Notice that when we applied Gram-Schmidt we first found an orthonormal basis
	for $W$, and since we know that $V = W \oplus W^\bot$ we get:
	\[ W^\bot = \Sp\{u_3\} = \Sp\left\{\frac{x}{\sqrt{2}}\right\} \]
	And as we know from the lectures for all $v\in V = \R_2[x]$ we get:
	\begin{align*}
		P_W(v) = \sum_{i=1}^{2}{\langle v,u_i \rangle u_i} &= 
		\langle v(x),\frac{x^2}{\sqrt{2}} \rangle \frac{x^2}{\sqrt{2}} + 
		\langle v(x),1-x^2 \rangle \left(1-x^2\right) \\ &= 
		\left(\sum_{x\in\{-1,0,1\}}
		{v(x)\left(\frac{x^2}{\sqrt{2}}\right)}\right)
		\left(\frac{x^2}{\sqrt{2}}\right) + 
		\left(\sum_{x\in\{-1,0,1\}}{v(x)(1-x^2)}\right)\left(1-x^2\right) \\ &= 
		\left(v(1)+v(-1)\right)\left(\frac{x^2}{2}\right) + v(0)
		\left(1-x^2\right)
	\end{align*}
	
	\newpage

	\subsection{Find the distance of $f(x)=x+1$ from $W$ according to each of
	the inner products.}
	We know that the distance of $f(x)=x+1$ from $W$ is the distance between
	$x+1$ and $P_W(x+1)$ which is the point ``closest'' to $x+1$ on $W$. So
	first we shall calculate $P_W(x+1)$ according to each of the inner product
	spaces:
	\begin{align*}
		P_W(x+1) &= \left(2+0\right)\left(\frac{x^2}{2}\right) + 1
		\left(1-x^2\right) = 1 \\
		P_W(x+1) &= \left(\int_{0}^{1}{(x+1)2x^2\,dx}\right) 2x^2 + 
		\left(\int_{0}^{1}{(x+1)\left(\frac{3}{2} - \frac{5}{2}x^2\right)\,dx}
		\right) \left(\frac{3}{2}-\frac{5}{2}x^2\right) \\ &=
		\left(\int_{0}^{1}{2x^3+2x^2\,dx}\right) 2x^2 +
		\left(\int_{0}^{1}{\frac{3}{2} - \frac{5}{2}x^2 + \frac{3}{2}x - 
		\frac{5}{2}x^3\,dx}\right) \left(\frac{3}{2}-\frac{5}{2}x^2\right) \\ &=
		\frac{7}{3}x^2 + \frac{19(3-5x^2)}{48} =
		\frac{112x^2}{48} + \frac{57 - 95x^2}{48} = \frac{17x^2+57}{48}
	\end{align*}
	So now according to $\langle ,\rangle_1$ we get that the distance is:
	\[
		\sqrt{\langle x+1,1\rangle} = \sqrt{\int_{0}^{1}{x+1\,dx}} = 
		\sqrt{\frac{3}{2}} = \frac{\sqrt{6}}{2}
	\]
	And now according to $\langle ,\rangle_2$ we get that the distance is:
	\[
		\sqrt{\langle x+1, \frac{17x^2+57}{48}\rangle} = 
		\sqrt{\sum_{x=-1,0,1}{\frac{17x^2+57(x+1)}{48}\,dx}} = 
		\sqrt{\frac{17+57+131}{48}} = \sqrt{\frac{205}{48}} = 
		\frac{\sqrt{615}}{12}
	\]
	\newpage

	\section{The one with the contraction}
	Let $V$ be a finite dimension inner product space and let $P\in\End(V)$ 
	be a contraction - that is $\forall v\in V(\|Pv\|\le\|v\|)$.
	
	\subsection{Show that $P$ is the orthogonal projection on its own image.}
	We will first show that $V = \im P \oplus \ker P$. Since $P$ is a projection
	we must have $P(v) = P^2(v)$ which implies $P(P(v)-v) = 0$ so 
	$P(v) - v = \epsilon \in \ker P$ and then $v = P(v) + (-\epsilon)$ which
	shows that $V = \im P + \ker P$. Now let $v\in \im P \cap \ker P$. We
	get that for some $u\in V$ that $P(u) = v$ and $P^2(u) = P(v) = 0$ since
	$v\in \ker P$. But since $P^2(u) = P(u)$ we get $v = 0$. This shows
	$V = \im P \oplus \ker P$. We also know that $V = \im P \oplus \im P^\bot$.
	This shows that $\dim\im P^\bot = \dim\ker P$. Now we will show that
	$\im P^\perp \subseteq \ker P$. Let $v\in\im P^\perp$ We know that 
	$P(v)\in\im P$ so:
	\[
		\langle P(v), v\rangle = 0
	\]
	This implies that:
	\[
		0 = \langle P(v), v\rangle = \frac{1}{4}
		\left(
			\|P(v)+v\|^2 - \|P(v)-v\|^2 + i\|P(v)-v\|^2 - i\|P(v)+v\|^2
		\right)
	\]
	This implies that:
	\[
		\|P(v)+v\| - \|P(v)-v\| = 0
	\]
	So using the reverse triangle identities we get:
	\[
		0 \le \|P(v)\| - \|v\| - \|P(v)-v\| \le \|P(v)+v\| - \|P(v)-v\| = 0
	\]
	So:
	\[
		\|P(v)\| - \|v\| = \|P(v)-v\|
	\]
	So from what we know $\|P(v)\| - \|v\|$ is a non-negative number and
	$\|P(v)\| \le \|v\|$ which implies  $\|P(v)\| - \|v\| = 0$ which gives:
	\[
		\|P(v)-v\| = 0 \Rightarrow P(v) - v = 0 \Rightarrow P(v) = v
	\]
	This shows that $v\in\im P$, and since $v\in\im P^\bot$ we know $v=0$.
	But we assumed that $v\notin\ker P$ so this can't be the case, and we get a
	contradiction. Which means that $\im P^\perp \subseteq \ker P$ and we know
	$\dim\im P^\bot = \dim\ker P$ so $\im P^\bot = \ker P$. so $P$ is an 
	orthogonal projection on its own image.
	
	\newpage

	\section{The one with the weird inequality}
	Let $V=\C_3[x]$ with the inner product 
	$\langle p(x),q(x)\rangle = \sum_{x=0}^{x=3}{p(x)\overline{q(x)}}$.
	
	\subsection{Find the minimal positive constant $C$ such that 
	for all $p\in V$:
	\[
	\| p(i) \| \le
	C\sqrt{\sum_{x=0}^{3}{\| p(x) \|^2}}
	\]}
	Notice that the following $\varphi \colon V \to \C$:
	\[
		\varphi(p(x)) = p(i)
	\]
	is a functional since for $\alpha\in\C$ and $p,q\in V$:
	\[
		\varphi(\alpha p + q) = (\alpha p + q)(i) = \alpha p(i) + q(i) = 
		\alpha \varphi(p) + \varphi(q)
	\]
	Using riesz representation theorem we get that exists $w$ such that:
	\[
		\varphi(p) = p(i) = \langle p, w\rangle
	\]
	Denote $w = a+bx+cx^2+dx^3$, 
	We see that for the basis $B = \{1,x,x^2,x^3\}$:
	\begin{align*}
		1 &= \varphi(1) = \langle 1, w\rangle = \overline{w(0)} + 
		\overline{w(1)} + \overline{w(2)} + \overline{w(3)} 
		= {w(0)} + {w(1)} + {w(2)} + {w(3)} = 1 \\
		i &= \varphi(x) = \langle x, w\rangle = 0\overline{w(0)} + 
		1\overline{w(1)} + 2\overline{w(2)} + 3\overline{w(3)} 
		\Rightarrow w(1)+2w(2)+3w(3) = -i \\
		-1 &= \varphi(x^2) = \langle x^2, w\rangle = 0\overline{w(0)} + 
		1\overline{w(1)} + 4\overline{w(2)} + 9\overline{w(3)} = 
		w(1 + 4w(2) + 9w(3)) = -1 \\
		-i &= \varphi(x^3) = \langle x^3, w\rangle = 0\overline{w(0)} + 
		1\overline{w(1)} + 8\overline{w(2)} + 27\overline{w(3)} \Rightarrow
		w(1) + 8w(2) + 27w(3) = i \\
	\end{align*}
	Solving this system of equations gives:
	\[
		(w(0),w(1),w(2),w(3)) = 
		\left(\frac{5}{3}i,\frac{5-5i}{2},-2+i,\frac{1}{2}-
		\frac{1}{6}i\right)
	\]
	And now we can solve for $p(i)$ for any $p\in V$. By Cauchy-Schwartz we get:
	\[
		\|p(i)\| = |\langle p,w\rangle| \le \|p(x)\|\|w(x)\| = 
		\sqrt{\sum_{x=0}^{3}{\| p(x) \|^2}} \sqrt{\sum_{x=0}^{3}{\| w(x) \|^2}}
	\]
	And we see that:
	\[
		\sqrt{\sum_{x=0}^{3}{\| w(x) \|^2}} = \frac{\sqrt{185}}{3}
	\]
	Since we know that the CS inequality can also be an equality we get that
	this is the minimal constant such that the inequality is satisfied and then:
	\[
		\boxed{C = \frac{\sqrt{185}}{3}}
	\]
	
	
	
	\newpage

	\section{The one with the invariance}
	Let $V$ be a finite dimension inner product space and $T\in\End(V)$. 
	
	\subsection{Show that $U\subseteq V$ is $T$-invariant iff $U^\bot$ is
	$T^*$-invariant}
	\underline{$U$ is $T$-invariant $\Rightarrow U^\bot$ is $T^*$-invariant:} \\ 
	Since $U$ is $T$-invariant we know that:
	\[
		T(U) \subseteq U
	\]
	Now suppose that $U^\bot$ is not $T^*$-invariant, that means that exists
	$u\in U^\bot$ such that $T^*(u)\notin U^\bot$, which means that:
	\[
		\langle v, T^*(u)\rangle \neq 0
	\]
	For some $v\in U$. This implies:
	\begin{align*}
		\langle T(v), u \rangle \neq 0
	\end{align*}
	But since $U$ is $T$-invariant we know that $T(v)\in U$, which implies
	that $u\notin U^\bot$ - that means that out assumption must be false
	so $U^\bot$ is $T^*$-invariant. \\
	\underline{$U$ is $T$-invariant $\Leftarrow U^\bot$ is $T^*$-invariant:} \\ 
	Since $U^\bot$ is $T^*$-invariant we know that:
	\[
		T^*(U^\bot) \subseteq U^\bot
	\]
	Now suppose that $U$ is not $T$-invariant, that means that exists
	$u\in U$ such that $T(u)\notin U$, which means that:
	\[
		\langle T(u), v\rangle \neq 0
	\]
	For some $v\in U^\bot$. This implies:
	\begin{align*}
		\langle u, T^*(v) \rangle \neq 0
	\end{align*}
	But since $U^\bot$ is $T^*$-invariant we know that $T^*(v)\in U^\bot$, 
	which implies that $u\notin U$ - that means that out assumption must be
	false so $U$ is $T$-invariant. \\
	
	\newpage

	\section{The one with $T*$}
	In the following sections find $T^*$

	\subsection{Let $(V,\langle , \rangle)$ be a finite dimension inner product
	space. Let $\alpha,\beta\in V$ and define $T=T_{\alpha,\beta}\in\End(V)$ 
	as such:
	\[
	T_{\alpha,\beta}(v) = \langle v, \alpha \rangle \beta
	\]}
	We see that for $T$ defined as such:
	\[
		\langle T(v), u \rangle = 
		\langle \langle v, \alpha \rangle \beta, u \rangle = 
		\langle v, \alpha \rangle \langle \beta, u \rangle = 
		\langle v, \alpha \overline{\langle \beta, u \rangle} \rangle = 
		\langle v, \alpha \langle u, \beta \rangle \rangle
	\]
	And since we know that:
	\[
		\langle T(v), u\rangle = \langle v, T^*(u)\rangle = 
		\langle v, \alpha \langle u, \beta \rangle \rangle
	\]
	We get that:
	\[
		T^*(u) = 
		\langle u, \beta \rangle \alpha
	\]

	\newpage

	\subsection{Let $V = (\Mat_n(\F),\langle,\rangle_{\std})$. 
	Let $Q\in\Mat_n(\F)$ be invertible and define $T=T_Q\in\End(V)$ as such:
	\[
	T_Q(A) = QAQ^{-1}
	\]}
	We see that from properties of trace:
	\begin{align*}
		\langle T(A), B \rangle &= 
		\langle QAQ^{-1}, B \rangle = \tr(QAQ^{-1}B^{t}) = 
		\tr(B^{t}QAQ^{-1}) \\ &= \tr(Q^{-1}B^{t}QA) = \tr(AQ^{-1}B^{t}Q) = 
		\langle A, (Q^{-1}B^{t}Q)^{t} \rangle
	\end{align*}
	And since we know that:
	\[
		\langle T(A), B\rangle = \langle A, T^*(B)\rangle = 
		\langle A, (Q^{-1}B^{t}Q)^{t} \rangle
	\]
	We get that:
	\[
		T^*(B) = 
		(Q^{-1}B^{t}Q)^{t} = Q^{t}B(Q^{-1})^{t}
	\]

	\newpage

	\subsection{Let $Tv=J_n(\lambda)v$ for $V=\F_n$ with 
	$\langle,\rangle_{\std}$}
	We see that for $T$ defined as such:
	\[
		\langle T(v), u \rangle = 
		\langle J_n(\lambda)v, u \rangle = (J_n(\lambda)v)^{t} u = 
		v^{t} J_n(\lambda)^{t} u = \langle v, J_n(\lambda)^{t} u \rangle
	\]
	And since we know that:
	\[
		\langle T(v), u\rangle = \langle v, T^*(u)\rangle = 
		\langle v, J_n(\lambda)^{t} u \rangle
	\]
	We get that:
	\[
		T^*(u) = 
		J_n(\lambda)^{t} u
	\]
	
	\newpage
	\section{The one with the adjoint operator}
	Let $a\in\C, |a| \neq 1$ and let $V$ be a finite dimension inner product
	space, $T\in\End(V)$
	\subsection{Show that if $T=aT^{*}$ then $T=0$}
	We first see that $T$ is normal since:
	\[
		TT* = aT^*T^* = T^*aT^* = T^*T
	\]
	This means that exists an orthonormal basis of eigenvectors of $T$ which
	we shall denote $B=(v_1,\dots,v_n)$ such that:
	\[
		[T]_B = \text{diag}(\lambda_1,\dots,\lambda_n)
	\]
	Where $\lambda_1,\dots,\lambda_n$ denote the corresponding eigenvalues.
	We see that for all $1\le i \le n$ that:
	\[
		T(v_i) = \lambda_i v_i
	\]
	But on the other hand that:
	\[
		T(v_i) = aT^*(v_i)
	\]
	We know from a theorem that if $v_i$ is an eigenvector of $T$ with 
	eigenvalue $\lambda_i$ then it is also an eigenvector of $T^*$ with
	eigenvalue $\overline{\lambda_i}$ so we get:
	\[
		\lambda_i v_i = a \overline{\lambda_i} v_i \Rightarrow
		\lambda_i = a \overline{\lambda_i}
	\]
	And in particular that:
	\[
		\vert \lambda_i \vert = \vert a \overline{\lambda_i} \vert \Rightarrow
		\vert \lambda_i \vert = \vert a \vert \vert \overline{\lambda_i} \vert
	\]
	But since also $\vert \lambda_i \vert = \vert \overline{\lambda_i} \vert$
	we get:
	\[
		\vert \lambda_i \vert (1-\vert a \vert) = 0
	\]
	And since $\vert a \vert \neq 1$ we get that $\lambda_i = 0$ which means
	that:
	\[
		[T]_B = 0
	\]
	So $T = 0$.
	
	\newpage
	
	\subsection{Show that if $T$ is normal then $\ker T = \ker(T - aT^{*})$}
	We can represent these tranformations and get that:
	\begin{align*}
		[T]_B &= \text{diag}(\lambda_1,\dots,\lambda_n) \\
		[T - aT^{*})]_B &= \text{diag}(\lambda_1 - a \overline{\lambda_1},\dots,
		\lambda_n - a \overline{\lambda_n})
	\end{align*}
	We know that the kernel of $v\in \ker(T)$ if and only if $v$ is in the span
	of $v_i$ with eigenvalue $0$, and that $v\in \ker(T - aT^{*})$ if and only
	if $v$ is in the span of $v_i$ with eigenvalue $0$ but we see:
	\begin{align*}
		\lambda_i = 0 &\Rightarrow \lambda_i = \overline{\lambda_i} = 0 
		\Rightarrow \lambda_i - a \overline{\lambda_i} = 0 \\
		\lambda_i - a \overline{\lambda_i} = 0 &\Rightarrow
		\lambda_i = a \overline{\lambda_i} \Rightarrow
		\vert \lambda_i \vert = \vert a \overline{\lambda_i} \vert \Rightarrow
		\vert \lambda_i \vert = \vert a \vert \vert \overline{\lambda_i} \vert
		\Rightarrow \vert \lambda_i \vert (1-\vert a \vert) = 0 \Rightarrow
		\lambda_i = 0
	\end{align*}
	Which shows that:
	\[
		\lambda_i = 0 \iff \lambda_i - a \overline{\lambda_i} = 0
	\]
	Which implies that the span of eignevectors from $B$ with eigenvalue
	$0$ in relation of $T$ will also have eigenvalue $0$ in relation to
	$T - aT^{*}$ so $\ker T = \ker(T - aT^{*})$ as wanted.
	
	
	\newpage
	
	\section{The one with the matrix}
	Given:
	\[
	A = 
		\begin{pmatrix}
		1 & -4 & 2 \\
		-4 & 1 & -2 \\
		2 & -2 & -2 
		\end{pmatrix}
	\]
	\subsection{Find an orthogonal matrix $O$ and a diagonal matrix $D$ such
	that $O^{T}AO=D$}
	We see that $A$ is symmetric so it must also be normal. From the spectral
	theorem for normal transformations we know that exists a basis $B$ to $V$
	such that $B$ is an orthogonal basis in realtion to the standard inner
	product and also comprises of eigenvectors of $A$. To find that $B$ we
	first will find the eigenvalues of $A$.
	\[
	A = 
		\left\vert
		\begin{pmatrix}
		1-\lambda & -4 & 2 \\
		-4 & 1-\lambda & -2 \\
		2 & -2 & -2-\lambda 
		\end{pmatrix}
		\right\vert = -(\lambda+3)(\lambda+3)(\lambda-6) = 0
	\]
	So the eigenvalues are $\lambda_1 = -3$ and $\lambda_2=6$. Now to find
	an orthogonal basis for $\ker(A-3)$ we do:
	\[
		\begin{pmatrix}
		4 & -4 & 2 \\
		-4 & 4 & -2 \\
		2 & -2 & 1 
		\end{pmatrix}
		\begin{pmatrix}
		x \\ y \\ z \\
		\end{pmatrix} = 0
	\]
	And we find that:
	\[
		\ker(A-3) = 
		\left\{
		a\begin{pmatrix} 1\\ 1\\ 0\\ \end{pmatrix} + 
		b\begin{pmatrix} -1\\ 0\\ 2\\ \end{pmatrix}
		\biggr\vert a,b\in\R
		\right\}
	\]
	So we can choose the orthonormal basis to be:
	\[
		B_{1} = 
		\left\{
		\frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 1\\ 0\\ \end{pmatrix}, 
		\frac{1}{\sqrt{18}} \begin{pmatrix} -1\\ 1\\ 4\\ \end{pmatrix}
		\right\}
	\]
	Now for $\ker(A+6)$ we do:
	\[
		\begin{pmatrix}
		-5 & -4 & 2 \\
		-4 & -5 & -2 \\
		2 & -2 & -8 
		\end{pmatrix}
		\begin{pmatrix}
		x \\ y \\ z \\
		\end{pmatrix} = 0
	\]
	And we find that:
	\[
		\ker(A+6) = 
		\left\{
		a\begin{pmatrix} 2\\ -2\\ 1\\ \end{pmatrix}
		\biggr\vert a\in\R
		\right\}
	\]
	So we can choose the orthonormal basis to be:
	\[
		B_{2} = 
		\left\{
		\frac{1}{3}\begin{pmatrix} 2\\ -2\\ 1\\ \end{pmatrix}
		\right\}
	\]
	We know that vectors of different eigenspaces are always orthogonal
	so we know that:
	\[
		B = B_{1} \cup B_{2} = 
		\left\{
		\frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 1\\ 0\\ \end{pmatrix}, 
		\frac{1}{\sqrt{18}} \begin{pmatrix} -1\\ 1\\ 4\\ \end{pmatrix}, 
		\frac{1}{3}\begin{pmatrix} 2\\ -2\\ 1\\ \end{pmatrix}
		\right\}
	\]
	And as we know from the unitary diagnolization theorem the orthogonal
	matrix that would diagonalize $A$ is the matrix with these columns so:
	\[
		O = \begin{pmatrix}
		\cfrac{1}{\sqrt{2}} & -\cfrac{1}{\sqrt{18}} & \cfrac{2}{3} \\
		\cfrac{1}{\sqrt{2}} & \cfrac{1}{\sqrt{18}} & -\cfrac{2}{3} \\
		0 & \cfrac{4}{\sqrt{18}} & \cfrac{1}{3}
		\end{pmatrix}
	\]
	And $D$ is just the matrix with the eigenvalues we found on the diagonal:
	\[
		D = \begin{pmatrix}
			-3 & 0 & 0\\
			0 & -3 & 0\\
			0 & 0 & 6\\
			\end{pmatrix}
	\]
	And:
	\[
		O^TAO = D
	\]
	
	\newpage
	
	\section{The one with the prove disprove}
	Let $T$ be an operator over a finite dimension inner product space. 
	Prove or disprove the following:
	\subsection{$T$ is unitary iff $T$ is invertible and exists an orthonormal
	basis $E$ such that $\|Te\|=1$ for all $e\in E$}
	This is false. Define $T\colon \R^2 \to \R^2$:
	\[
		T(1,0) = (1,0) \quad\text{and}\quad 
		T(0,1) = \left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)
	\]
	We can see that it is invertible, and exists the standard basis $E$ 
	which is orthonormal such that $\|T(e1)\| = \|T(e2)\| = 1$, yet if we 
	consider $T(1,1)$ we see that:
	\[
		\|(1,1)\| = \sqrt{2} \neq 
		\sqrt{2 + \sqrt{2}} = 
		\left\|\left(1+\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)\right\| = 
		\|T(1,1)\|
	\]
	So we found a vector $v = (1,1)$ such that:
	\[
		\|v\| \neq \|T(v)\|
	\]
	Which means that $T$ isn't unitary.
	
	
	\newpage
	
	\subsection{$T$ is unitary iff $\|Tv\|=1$ for all $v\in V$ such that 
	$\|v\|=1$}
	$(\Rightarrow)$ \\
	Let $T$ be unitary, then we know that for any $v\in V$ such that $\|v\| = 1$
	that:
	\[
		\|Tv\| = \|v\| = 1
	\]
	$(\Leftarrow)$ \\
	Suppose that $v'$ is an eigenvector of $T$ with eigenvalue $\lambda$.
	We can normalize $v'$ and consider:
	\[
		v = \frac{v'}{\|v'\|}
	\]
	This vetcor is also an eigenvector of $T$ with eigenvalue $\lambda$ so
	\[
		T(v) = \lambda v
	\]
	But since $\|v\| = 1$ we also know that:
	\[
		\|T(v)\| = \|\lambda\| \|v\| = 1 \Rightarrow \|\lambda\| = 1
	\]
	And we know that if for any eigenvalue $\lambda$ of $T$ that 
	$\|\lambda\|=1$  then $T$ is unitary. That means that we have just shown
	that $T$ is unitary.
	
	\newpage
	
	\subsection{$T$ is unitary iff for all orthonormal vectors $v,u$ then 
	$Tv,Tu$ are also orthonormal}
	This is true. From the Gram-Schmidt theorem we know that exists 
	$B=(v_1,\dots,v_n)$ an orthonormal basis for $V$, since any two vectors 
	$u,v\in B$ are orthonormal we get that any $T(u),T(v)\in T(B)$ are also 
	orthonormal. So the set $T(B)$ is also orthonormal. Suppose it werent 
	linearly independent we get that exist $(a_1,\dots,a_n)\neq 0$ such that:
	\[
		\sum_{i}{a_i T(v_i)} = 0
	\]
	Using Parseval's identity we get that:
	\[
		\left\|\sum_{i}{a_i T(v_i)}\right\| = 
		\sqrt{\sum_{i}{\|a_i\|}} = \|0\| = 0
	\]
	But this can only happen if $\forall i(a_i = 0)$ so $T(B)$ is linearly
	indepenedent and we got that $T$ sends the orthonormal basis $B$ to 
	$T(B)$ an orthonormal basis. Let $v=\sum_{i}{a_i v_i}\in V$ we see that 
	using Parseval's identity twice gives:
	\[
		\|T(v)\| = \left\|T\left(\sum_{i}{a_i v_i}\right)\right\| 
		= \left\|\sum_{i}{a_i T(v_i)}\right\| = 
		\sqrt{\sum_{i}{\|a_i\|}} = \|v\|
	\]
	We know that this is equivalent to $T$ being unitary which completes the
	proof.
	
	\newpage
	
	\section{The one with the inequality}
	Let $T$ be a operator over an inner product space $V$ and let 
	$TT^{*}=\alpha T + \beta I$ for some $\alpha,\beta\in\R$.
	\subsection{Show that $\alpha^2+4\beta \ge 0$}
	\underline{case $a=0$}
	\[
		TT^* = \beta I
	\]
	So \[ \beta T^{-1} = T^* \] This implies??? \\ 
	\underline{case $a \neq 0$} \\
	We know that $TT^*$ is self-adjoint, and since $\alpha,\beta\in\R$ 
	we get that:
	\[
		\alpha T + \beta I = (TT^*)
		= (TT^*)^* = (\alpha T + \beta I)^* = \alpha T^* + \beta I
	\]
	Because $a\neq 0$ we get:
	\[
		T = T^*
	\]
	Which means that:
	\begin{align*}
		T^2 &= \alpha T + \beta I \\ \Rightarrow 
		p(T) &= T^2 -  \alpha T - \beta I = 0
	\end{align*}
	This implies???
	
	\newpage
	
	\section{The one with the square root}
	Let $T$ be a self-adjoint operator over a finite inner product space.
	\subsection{Prove that exist non-negative operators $A,B$ such that:
	\[
		T = A-B,\quad \sqrt{TT^{*}}=A+B,\quad AB=BA=0
	\]}
	We know that if $T$ is self-adjoint which implies it is unitary 
	diagonalizable over $\R$, so exist $O\in O(n)$ and $D$ diagonal such that:
	\[
		O^{T}DO = [T]_C
	\]
	For $C$ the basis with the $i$th vector being the $i$th column of $O$.
	Since $T$ is self-adjoint we know that all of eigenvalues are real.
	We can denote them by the entries of the main diagonal of $D$ as such:
	$\lambda_i = D_{ii}$, and now we can define two matrices:
	\[
		(A')_{ij} = 
		\begin{cases}
			D_{ii} & i=j \land D_{ii} \geq 0 \\
			0 & \text{otherwise}
		\end{cases}
	\]
	And:
	\[
		(B')_{ij} = 
		\begin{cases}
			-D_{ii} & i=j \land D_{ii} \leq 0 \\
			0 & \text{otherwise}
		\end{cases}
	\]
	And define the operator $A,B$ as such:
	\[
		A(v) = (O^{T} A' O)(v) \quad\text{and}\quad B(v) = (O^{T} B' O)(v)
	\]
	We see that $A,B$ are self-adjoint since $O^* = O^{T}$ and since all of
	their eigenvalues by construction are non-negative we know that they are
	non-negative operators. We may notice that:
	\[
		A - B = O^{T} A' O - (O^{T} B' O) = O^{T} (A'-B') O = O^{T} D O = T
	\]
	And also that:
	\[
		\sqrt{TT^*} = \sqrt{O^{T} DD^* O} = O^{T} |D| O = O^{T} (A'+B') O = 
		A+B
	\]
	And since diagonal matrices commute under matrix multiplication and also
	$O^{T} = O^{-1}$ we see:
	\[
		AB = BA = A'B' = 0
	\]
	Since $A'$ multiplies all the rows different than $0$ in $B$ and all the rows
	that are zero in a scalar. This completes the proof.
	
	\newpage
	\section{The one with the polynomial}
	Let $T$ be a self-conjugate polynomial over the inner product space $V$, 
	with eigenvalues $\lambda_1,\dots,\lambda_n$.
	\subsection{For any $p(x)\in\F[x]$ show that the singular values of $p(T)$ 
	are $\vert p(\lambda_i)\vert$ up to inner order.}
	Since $p(x)$ is a polynomial we can write:
	\[
		p(x) = a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0
	\]
	And:
	\begin{align*}
		p(T) &= a_nT^n+a_{n-1}T^{n-1}+\cdots+a_0I \\
		p(T)^* &= (a_nT^n)^* + (a_{n-1}T^{n-1})^* + \cdots + (a_0I)^* = 
		\overline{a_n}(T^*)^n + \overline{a_{n-1}}(T^*)^{n-1} + \cdots + 
		\overline{a_0}I
	\end{align*}
	Let $\lambda$ be an eigenvalue asscociated with an eigenvector $v$ of $T$. 
	We see that:
	\begin{align*}
		p(T)(v) &= (a_nT^n+a_{n-1}T^{n-1}+\cdots+a_0I)(v) \\ &= 
		a_nT(v)^n + a_{n-1}T(v)^{n-1} + \cdots + a_0I(v) \\ &= 
		a_n\lambda^{n}v + a_{n-1}\lambda^{n-1}v + \cdots + a_0v \\ &= 
		p(\lambda)(v)
	\end{align*}
	And:
	\begin{align*}
		p(T)^*(v) &= 
		(\overline{a_n}(T^*)^n + \overline{a_{n-1}}(T^*)^{n-1} + \cdots + 
		\overline{a_0}I)(v) \\ &=
		\overline{a_n}(T^*(v))^n + \overline{a_{n-1}}(T^*(v))^{n-1} + \cdots + 
		\overline{a_0}v\\ &=
		\overline{a_n\lambda^n}v + \overline{a_{n-1}\lambda^{n-1}}v + \cdots + 
		\overline{a_0}v\\ &=
		\overline{p(\lambda)}v
	\end{align*}
	So the eigenvalues of $p(T)^*p(T)$ are exactly 
	$\overline{p(\lambda)}p(\lambda)$ which is exactly $\|p(\lambda)\|^2$.
	By SVD we know that the singular values of $p(T)$ are the square roots
	of the eigenvalues of $p(T)^*p(T)$, or in other words, the singular values
	of $p(T)$ are $\|p(\lambda_i)\|$ up to order.
	
	\newpage
	
	\section{The one with the operator norm}
	\subsection{Show that $\|T^*T\|_{\text{op}} =  \|T\|_{\text{op}}^{2}$}
	We know that:
	\[
		\|T\|_{\mathrm{op}} = \sup_{\|x\|=1}{\|Tv\|} 
					= \sup_{\|x\|=1}{\sqrt{\ip{T(v)}{T(v)}}}
					= \sup_{\|x\|=1}{\sqrt{\ip{T^*T(v)}{v}}}
	\]
	From this follows that:
	\[
		\|T\|_{\op}^{2} = \sup_{\|x\|=1}{\ip{T^*T(v)}{v}}
	\]
	We may notice that $\ip{T^\ast Tv}{v}$ is a non-negtive number since it's
	just the norm of $\ip{Tv}{Tv}$ which means using Cauchy-Schwartz we get:
	\[
		\|T\|_{\op}^{2} = \sup_{\|x\|=1}{\ip{T^*T(v)}{v}} \le 
		\|T^\ast T\| \|x\| = \|T^\ast T\|
	\]
	So we got that $\|T\|_{\op}^{2} \le \|T^\ast T\|$. To prove the other
	direction we recall that we saw in the rehearsal that $T$ and $T^\ast$
	have the same singular values and in particular that:
	\[
		\|T\|_{\op} = \|T^\ast\|_{\op}
	\]
	So using this and properties of the norm we get:
	\[
		\|T^\ast T\| \le \|T^\ast\| \|T\| = \|T\|^2
	\]
	From this and the other inequality we get:
	\[
		\boxed{\|T^*T\|_{\text{op}} =  \|T\|_{\text{op}}^{2}}
	\]
	
	\newpage
	
	\section{The one with the reflexive bilinear form}
	Let $f$ be a reflexive bilinear form over a finite dimension $V$.
	\subsection{Show that if $\mathrm{rank}f=r$ then exist 
	$\phi_1,\tau_1,\dots,\phi_r,\tau_r\in V^*$ such that:
	\[
		f(x,y) = 
		\phi_{1}(x)\tau_{1}(y) + \cdots + \phi_{r}(x)\tau_{r}(y)
	\]}
 	We know that $\mathrm{rank} f = r$ so if we denote $A = [f]_B$ 
 	we get that $\dim\ker(A) = n - r$. Denote the basis for the
 	kernel at $B_k = \{e_{n-r+1}, \dots, e_n\}$ and complete it
 	to a basis for $V$ as such $B = \{e_{1}, \dots, e_n\}$ 
 	Now for each $u,v\in\mathrm{span}\{e_{1}, \dots, e_r\}$ we can 
 	denote:
	\begin{align*}
		u = \sum_{i=1}^{n}{\alpha_i e_i} \\
		v = \sum_{i=1}^{n}{\beta_i e_i}
	\end{align*}
	And now for any $u,v$ we see:
	\[
		f(u,v) = f\left(\sum_{i=1}^{n}{\alpha_i e_i}, v\right) = 
		\sum_{i=1}^{n}{\alpha_i f(e_i,v)} = 
		\sum_{i=1}^{r}{\alpha_i f(e_i,v)}
	\]
	The last equality is true since we know that:
	\[
		f(v,e_i) = [v]_B A [e_i]_B = [v]_B 0 = 0
	\]
	And since $f$ is reflexive we get $f(e_i,v) = 0$ as well.
	Let $\phi_i\in V^*$ where $1 \le r \le n$ be defined as:
	\[
		\phi_i\left(\sum_{j}{\alpha_j e_j}\right) = \alpha_i
	\]
	And:
	\[
		\tau_i(v) = f(e_i,v)
	\]
	These are trivially linear functionals. From the above calculations we see that:
	\[
		f(u,v) = \sum_{i=1}^{r}{\phi_i(u)\tau_i(v)}
	\]
	Which is what we wanted to prove.
	
	
	
	
	\newpage
	
	\section{The one where we show some things are unique}
	Let $V$ be a finite dimension inner product space over $\R$, $f$ be a 
	bilinear form over $V$.
	\subsection{Show that exists a unique $T\in\End(V)$ such that:
	\[
		f(u,v) = \ip{u}{T(v)}, \quad \forall u,v\in V
	\]}
	We know by Gram-Schmidt that $V$ has an orthonormal basis $B$ which 
	implies:
	\[
		\ip{v}{u} = \ip{[v]_B}{[u]_B}_{\std}
	\]
	So we need to show that exists a unique $T\in\End(T)$ such that:
	\[
		f(u,v) = \ip{[u]_B}{[T(v)]_B}_{\std}, \quad \forall u,v\in V
	\]
	Let:
	\[
		[T(v)]_B = [f]_B [v]_B \in \End(T)
	\]
	We see that:
	\[
		f(u,v) = [u]_B^* [f]_B [v]_B = [u]_B^* [T(v)]_B
		= \ip{[u]_B}{[T(v)]_B}_{\std}
	\]
	This shows that exists a $T$ as wanted, we will now show it's unique.
	Let $S\neq T$ such that:
	\[
		\ip{u}{T(v)} = \ip{u}{S(v)}
	\]
	From this follows that:
	\begin{align*}
		\ip{u}{T(v)} - \ip{u}{S(v)} &= 0 \\ \Rightarrow
		\ip{u}{T(v)-S(v)} &= 0 \\ \Rightarrow
		\ip{u}{(T-S)(v)} &= 0
	\end{align*}
	Since $T \neq S$ exists $v'$ such that $(T-S)(v')\neq 0$ and for all 
	$u\in V$ and specifivally for $T(v')$ we get:
	\[
		\ip{T(v')}{(T-S)(v)} = \ip{T(v')}{T(v')} = \|T(v')\|^2 = 0
	\]
	But since $T(v') \neq 0$ this can't be. This implies that $T$ is indeed
	unique.
	
	\newpage
	\section{The one with the inner product}
	Let $A\in \mathrm{Mat}_n(\R)$ be symmetric and also satisfy:
	\[
		(A^2-5A+7I)^3 = I
	\]
	\subsection{Show that:
	\[
		f(x,y) = x^{T} A y
	\]
	is an inner product on $\R^2$}
	To show that this is an inner product on $\R^2$ we need to show that $f$
	is positive-definite. Since $A$ is symmetric and real it is self conjugate.
	By a theorem from class we know that if it is self conjugate and all of 
	its eigenvalues are positive then $A$ is positive definite and then
	$f$ is an inner product. Let $\lambda$ be an eigen value of $A$ with
	a corresponding eigenvector $v_\lambda$ such that:
	\[
		Av_{\lambda} = \lambda v_{\lambda}
	\]
	Since $A$ satisfies the above equality we see that:
	\[
		v_{\lambda} = Iv_{\lambda} = (A^2-5A+7I)^3 v_{\lambda} = 
		(A^2-5A+7I)^2 
		(\lambda^2 v_{\lambda} - 5\lambda v_{\lambda} + 7 v_{\lambda}) = 
		(A^2-5A+7I)^2 (\lambda^2 - 5\lambda + 7)v_{\lambda}
	\]
	Consider the real polynimial $g(x) = x^2-5x+7$. We see that its discriminant
	is $\sqrt{25 - 28}$ which means it doens't have any roots. Since the
	coefficient of $x^2$ is positive that means that $g(x) > 0$ for any
	real $x$ and specifically that $g(\lambda) > 0$ which gives:
	\[
		(A^2-5A+7I)^3 v_{\lambda} = 
		(A^2-5A+7I)^2 g(\lambda) v_{\lambda} = 
		(A^2-5A+7I)   g(\lambda)g(\lambda) v_{\lambda} = 
					  g(\lambda)g(\lambda)g(\lambda) v_{\lambda}
	\]
	This implies that $1 = g(\lambda)^3$. The only real solution to that
	equation is $g(\lambda) = 1$, considering the equation $g(x) = 1$
	we see:
	\[
		g(x) = 1 \Rightarrow x^2-5x+7 - 1 = 0 \Rightarrow (x-2)(x-3) = 0
	\]
	So $\lambda = 2$ or $\lambda = 3$. This implies that all the eigenvalues
	of $A$ are positive and as we said that implies that $f$ is an inner
	product and completes the proof.
	
	\newpage
	
	\section{The one with equivalence}
	\subsection{How many bilinear forms are there over $\R^2$ for which 
	exists $0\neq x\in\R^2$ such that $f(x,x) > 0$ up to isomorphism?}
	Let $B$ be a bilinear form and $E$ be a basis for $\R^2$. We know
	that each bilinear form defines a quadratic form $q$. We also
	know that any quadratic form can be represented by a symmetric matrix $S_q$. 
	Since $S_q$ is symmteric we can use Sylvester's law of inertia and get that 
	each $S_q$ is uniquely congruent to a matrix of the form:
	\[
		I_{n_+} \oplus -I_{n_-} \oplus O_{n_0}
	\]
	We now need to consider all the options that are not negative semi-definite
	so there would be an $x \neq 0$ such that $f(x,x) > 0$.
	Since we are talking about a $2 \times 2$ matrix here there are only $5$
	such options:
	\begin{align*}
		\begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			1 & 0 \\
			0 & 0
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			0 & 0 \\
			0 & 1
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			1 & 0 \\
			0 & -1
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			-1 & 0 \\
			0 & 1
		\end{pmatrix}
	\end{align*}
	So there are exactly $5$ bilinear forms over $\R^2$ for which exists $x\neq0$
	such that $f(x,x) > 0$ up to isomorphism.
	
	
	\newpage
	
	\section{The one with the two}
	Let $f$ be a symmetric bilinear form over a real finite-dimension vector 
	space $V$.
	\subsection{Prove that if $W\subseteq V$ is a subspace such that $f\vert_W$ 
	is positive definite, then $\dim W \le n_{+}(f)$}
	Denote $\dim (W) = k$ and
	let $B_W = (v_1,\dots,v_{k})$ be a basis for $W$, and
	$B_v = (v_1,\dots,v_{k},\dots v_n)$ be a basis for $V$.
	We know that $f$ is a symmteric bilinear form, which implies that $[f]_B$ is 
	symmetric. So by Sylvester's law of inertia we get that exists a diagonal 
	matrix $D$ and an invertible matrix $S$ such that $[f]_B$ is congruent to $D$ 
	and:
	\[
		S^{T}[f]_BS = D
	\]
	We also know by the orthogonal diagonalization theorem for real symmetric
	matrices that exists $O\in O(n)$ such that:
	\[
		O^{T}[f]_BO = D'
	\]
	Where $D$ is diagonal with the eigenvalues of $[f]_B$ on its diagonal.
	Since we know that $f\vert_W$ is positive definite that means that all
	of its eigenvalues are positive and moreover that $D'_{11},\dots,D'_{kk}$
	are the eigenvalues of $W$ and thus positive.
	Since the positive values on the diagonal corresponds to $n_+(D')$
	we get that $n_+(D') \geq \dim W$ and since Sylvester's character
	and the rank don't change between congruent matrices
	\footnote{Notice that $D$ and $D'$ are congruent because congruency
	is an equivalence relation} 
	we get that
	$n_+(f) \geq \dim W$ too, which is exactly what we wanted to prove.
	
	\newpage
	
	\subsection{Let $B = (b_1,\dots,b_n)$ be a Sylvester basis such that:
	\[
		[f]_B = I_{n_{+}} \oplus (-I_{n_{-}}) \oplus O_{n_{0}}
	\]
	Does it necessarily follow that 
	$W\subseteq \mathrm{sp}\{b_1,\dots,b_{n_{+}}\}$}
	No. Let $V = \R^2$ and $E = \{e_1,2e_2\}$ be a basis to $\R^2$ such that
	$e_1,e_2$ are the vectors from the standard basis and:
	\[
		[f]_B = 
		\begin{pmatrix}
			1 & 0 \\
			0 & -1 \\
		\end{pmatrix}
	\]
	We see that $E$ is a Sylvester's basis but if we consider:
	\[
		W = \mathrm{sp}\{(1,1)\}
	\]
	Then $W$ is indeed a linear subspace of $V$ and if we let $w=(a,a)\in W$
	we see that:
	\[
		\ip{[f]_B[w]_B}{[w]_B} = 
		\begin{pmatrix}
		2a & a
		\end{pmatrix}
		\begin{pmatrix}
			1 & 0 \\
			0 & -1 \\
		\end{pmatrix}
		\begin{pmatrix}
		2a \\ a
		\end{pmatrix} = 3a^2
	\]
	And of course $f\vert_B$ is also symmetric so by a theorem it is 
	positive definite, yet as we can easily see $W \nsubseteq \mathrm{sp}\{e_1\}$
	
	\newpage
	
	\section{The one where we prove... or disprove?}
	Let $A$ be a symmetric real matrix of order $n\times n$ over $V$.
	\subsection{$A$ is non-negative iff $\Delta_i(A) \geq 0$ for all 
	$i = 1,\dots,n$. Consider both directions}
	\underline{$(\Leftarrow)$} \\
	This is false because we can look at the matrix over $\R$:
	\[
	A = 
		\begin{pmatrix}
			0 & 0 \\
			0 & -1
		\end{pmatrix}
	\]
	We see that:
	\[
		\Delta_1(A) = 0 \quad\text{and}\quad \Delta_2(A) = \det(A) = 0
	\]
	But still we see that is is symmetric and it has a negative eigenvalue.
	\\
	\underline{$(\Rightarrow)$} \\
	Assume that $A$ is non-negative. This clearly implies that any principle 
	minor corresponding to $\Delta_i(A)$ is also non-negative, which means that
	all of its eigenvalues are non-negative. Since the determinant of any
	principle minor is the product of its eigenvalues we get that for all
	$i = 1,\dots,n$ that $\Delta_i(A) \geq 0$ which is what we wanted to prove.
	
	\newpage
	
	\iffalse
	\section{The one with the extra difficulty}
	\subsection{Let $f$, $g$ be inner products on $V$ a finite-dimension vector 
	space. Prove that exists a basis that is orthonormal by $f$ and orthogonal 
	by $g$.}
	\fi
	
	
	
	
	
	
\end{document}