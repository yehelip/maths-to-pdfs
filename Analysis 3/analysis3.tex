\documentclass[11pt,a4paper]{article}
\setlength {\marginparwidth }{2cm}
\usepackage{amssymb,amsfonts,amsmath,calc,tikz,pgfplots,geometry}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{todonotes}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usetikzlibrary{positioning}
\pgfplotsset{compat=1.18}
\setlength{\headheight}{14.6pt}
\addtolength{\topmargin}{-1.6pt}
\geometry{margin=1in}
\hypersetup{
    colorlinks=false, %set true if you want colored links
    linktoc=all,   %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\idealin}{\triangleleft}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\Out}{Out}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\st}{\, s.t. \,}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Omicron}{O}
\newcommand{\ip}[2]{\langle #1, #2 \rangle}
\newcommand{\set}[2]{ \left\{ #1 \mid #2 \right\} }
\newcommand{\bigslant}[2]
{{\raisebox{.2em}{$#1$}\left/\raisebox{-.2em}{$#2$}\right.}}
\renewcommand{\tt}[1]{\textnormal{\textbf{(#1).}}} %tt=theorem title
\newcommand{\abs}[1]{\left\lvert #1\right\rvert}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\textbf{Analysis Notes}}
\author{yehelip}
\date{}
\usepackage{amsmath}
\begin{document}
	\maketitle
	\newpage
	\section{Introduction to Topology in Euclidean Spaces}
	\subsection{Norms and Metrics}
  We will start by giving basic topological definitions in the eclidean
	space $\R^d$. First we define
	\[
		\R^d := \set{(x_1,x_2,\dots,x_d)}{\substack{1 \le i \le d \\ x_i\in\R}}
	\]
	And now we can continue to define some more topological terms:
	\begin{definition}
	The \textbf{Eulidean norm} is defined as:
	\[
		\|x\| = \|x\|_2 := \sqrt{\sum_{i=1}^{d}{x_i^2}}
	\]
	We can similarly define the $L_p$ norm as:
	\[
		\|x\| = \|x\|_p := \sqrt[p]{\sum_{i=1}^{d}{x_i^p}}
	\]
	\end{definition}
	\noindent Which satisfies all properties of the norm
	\begin{definition}
	The \textbf{Euclidean metric} is defined as:
	\[
		d_2(P_1,P_2) := \|P_1-P_2\|_2 = \sqrt{(x_1-x_2)^2 - (y_1-y_2)^2}
	\]
	\end{definition}
	\noindent Notice that it is induced by the Euclidean norm and similarly
	we can induce $L_p$ metric using $L_p$ norms.
	\subsection{Sequences}
	Up until now we didn't have many problems using subscript for indexes
	of sequences, but now since we have coordinates we must denote a sequence
	in another similar way called $^{\text{superscript}}$ as such 
  $(x^n)_{n=1}^{\infty}$. We define convergence of such sequences in
  the following way:
	\[
		\lim_{n\to\infty}{x^n} = x 
		\quad \iff \quad
		\forall i\left(\lim_{n\to\infty}{x_i^n} = x_i\right)
	\]
	\begin{definition}
	A sequence $(x^n)_{n=1}^{\infty}$ is called a Cauchy sequence if and only 
	if:
	\[
		\lim_{n,m\to\infty}{\|x^n-x^m\|} = 0
	\]
	\end{definition}
	
	\newpage
	
	\subsection{Definitions}
	\begin{definition}
	A \textbf{complete metric space} is a metric space $M$ such that every
	Cauchy sequence in $M$ converges to some limit in $M$.
	\end{definition}
	\begin{definition}
	An \textbf{open set} in a Euclidean space is a subset $U$ such that
	for any $x \in U$ exists $\varepsilon > 0$ such that any $y$ such that
	any $y \in B_\varepsilon(x)$ satisfies $y\in U$
	\end{definition}
	\begin{definition}
	In a topological space $X$ a space a \textbf{neighborhood} of a point 
	$x\in X$ is a subset such that exists an open set $U$ such that 
	$p\in U\subset V$
	\end{definition}
	\begin{definition}
	A \textbf{close set} $E$ in a Euclidean space $X$ is a subset of $X$ such
	that:
	\[
		(x^n)_{n=1}^{\infty}\subseteq E \quad x^n \xrightarrow{n\to\infty} x
		\implies x \in E
	\]
	\end{definition}
	\begin{definition}
	A topological space is called \textbf{compact} if every open cover of 
	$X$ has a finite subcover.
	\end{definition}
	\begin{definition}
		A topological space is called \textbf{sequentially compact} if
		every sequence $(x^n)_{n=1}^{\infty}$ has a subsequence 
		$(x^{n_k})_{k=1}^{\infty}$ that converges to a point $x$ in the space.
	\end{definition}
	\noindent
	In Euclidean spaces, being sequentially compact is equivalent to being
	compact. Let $E$ be a subset of a Euclidean space.
	\begin{definition}
	The \textbf{closure} of $E$ is defined as:
	\[
		\mathrm{Cl}(E) = \set{x}{\exists (x^n) \colon 
    x^n \xrightarrow{n\to\infty} x}
	\]
	\end{definition}
	\begin{definition}
	The \textbf{interior} of $E$ is defined as:
	\[
		\mathrm{Int}(E) = \set{x}{\exists r > 0 \colon B_r(x) \subseteq X}
	\]
	\end{definition}
	\begin{definition}
	The \textbf{boundary} of $E$ is defined as:
	\[
		\partial E = \set{x\in\R^d}{\forall r > 0\
		\exists x \in X \land \exists y \in X^c\ : y,z\in B_r(x)}
	\]
	\end{definition}
	\begin{definition}
	A function is \textbf{continuous} at $x\in X$ if for every $\varepsilon > 0$
	exists $\delta > 0$ such that for every $\|x-y\| < \delta$
	\[
		\|f(y) - f(x)\| < \varepsilon
	\]
	And we say that a function is continuous on $X$ if it is continuous
	for every $x\in X$
	\end{definition}
	An important, equivalent, more general definition for continuity is that
	if $f$ is a function from $A$ to $B$ then if $U$ is an open set in $B$
	implies $f^{-1}(U)$ is an open set we say that $f$ is continuous on $A$.
	\begin{definition}
	A function is \textbf{uniformly continuous} on $X$ if for every 
	$\varepsilon > 0$ exists $\delta > 0$ such that for every 
	$\|x-y\| < \delta$
	\[
		\|f(x) - f(y)\| < \varepsilon
	\]
	\end{definition}
	\begin{definition}
	A \textbf{connected} space is a topological space that cannot be 
	represented as the union of two or more disjoint nonempty open subsets. 
	\end{definition}
	\begin{definition}
	A \textbf{path} from $x\in X$ to $y\in X$ is a continuous function 
  $f \colon [0,1] \to X$ such that $f(0) = 0$ and $f(1) = y$.
	\end{definition}
	\begin{definition}
	A \textbf{path connected} space $X$ is a topological space such that 
	exists a path between any two points in $X$.
	\end{definition}
	
	\newpage
	
	\section{Practice}
	Phew! These were a lot of definitions... Now it's time for some practice!
	\\ \textbf{Prove that a continuous function $f\colon A\to B$ has 
	a maximum in a compact space} \\
	By the completeness axiom for the real numbers we know that the set
	$f(A)$ has a supremum which we will denote $S$. By the definition
	of the supremum it is possible to construct a sequence that converges
	to it which we shall denote $f(x^n)$. We don't know whether $x^n$
	converges or not but we know it has a subsequence that converges so:
	\[
		\lim_{k\to\infty}{x^{n_k}} = x \quad \text{and} \quad 
		\lim_{k\to\infty}{f(x^{n_k})} = S
	\]
	Since $f$ is close we get $x\in A$ and since it is continuous $f(x) = S$
	which shows that it is continuous and also has a maximum. \\
	\textbf{Prove that a set $E$ is closed if and only if it's complement
	$E^c$ is open} \\
	$(\Rightarrow)$ \\
	Suppose that $E$ is closed, and $E^c$ is not open. Then exists
	$x\notin E$ such that for all $r > 0$ we get $B_r(x) \cap E \neq 
	\emptyset$ which means we can construct a sequence in $E$ that converges
	to $x$ but $x\notin E$ in contradiction to the assumption that $E$ is 
	close. \\
	$(\Leftarrow)$ \\
	Suppose that $E^c$ is open and $E$ is not closed, then exists a sequence
	$(x^n)_{n=1}^{\infty}$ that converges to some $x \in E^c$ which means
	that for every $r > 0$ that $B_r(x) \cap E \neq \emptyset$ in 
	contradiction to $E_c$ being open.
	
	\newpage
	
	\section{Differentiability}
	Let $A \in \R^{m \times n}$. We define the linear map 
	$T_A \colon \R^n \to \R^m$ by:
	\[
		T(x) = Ax \quad x\in \R^n
	\]
	Let $T \colon V \to M$ be a linear transformation between inner product
	spaces, we define the operator norm to be:
	\[
		\|T\|_{\mathrm{op}} = \|T\| = \sup_{v \neq 0}{\frac{\|Tv\|_W}{\|v\|_V}}
	\]
	and $T$ is said to be bounded if $\|T\| < \infty$. An important result
	to prove is that if $T$ is a bounded linear transformation then:
	\[
		\|T\|_{\mathrm{op}} \le 
		\left(\sum_{i,j}{}a_{ij}^{2}\right)^{\frac{1}{2}} < \infty
	\]
	and since this gives that:
	\[
		\|T(x) - T(y)\| \le \|T\|_{\mathrm{op}}\|x-y\|
	\]
	we get that $T$ is continuous and even Lipschitz continuous on its domain.
	\begin{definition}
	An \textbf{affine function} is a function fo the form:
	\[
		T_{A,b}(x) = Ax + b
	\]
	such that $A\in\R^{m \times n}$ and $b\in x\in \R^{m}$.
	\end{definition}
	\noindent Many basic properties of derivatives also satisfied by this
	definition of differentiablity. Now to define a general directional
	derivative:
	\begin{definition}
		Let $f \colon U \to \R$ with $U \subset \R^n$, $a \in U$, $0 \neq v
		\in \R^n$. Then, if the following limit exists:
		\[
			D_v f(a) = \lim_{t \to 0}{\frac{f(a + tv) - f(a)}{t}}
		\]
		Then we say that $f$ is differentiable in the direction of $v$ and
		$D_v f(a)$ is called the \textbf{directional derivative} of $f$ at $a$.
	\end{definition}
	\noindent
	Note that if $\mathcal{E} = \{e_1,\dots,e_n\}$ is the standard basis
	for $\R^n$ then we may denote the directional derivative of $f$ at $a$
	in the direction of $e_i$ in one of the following ways:
	\[
		D_{e_i} f(a) = D_i f(a) = \frac{\partial f}{\partial e_i}(a) = 
		f_{x_i}(a)
	\]
	\begin{proposition}
		If $f$ is differentiable at $a$ then:
			\[
				D_v f(a) = D f(a) v
			\]	
		For every $v \in \R^n$.
	\end{proposition} 
	\noindent Try proving this using the fact that:
	\[
		f(a+h) = f(a) + Df(a)h + o(h)
	\]
	An important corollary of this proposition is:
	\[
		D_vf(a) = D f(a) v = \nabla f(a) \cdot v = 
		\sum_{i}^{n}{\frac{\partial f}{\partial x_i}(a)v_i}
	\]
	
	\newpage
	\noindent Notice that we get the biggest value for $\|v\| = 1$ if
	$v = \frac{\nabla f(a)}{\|\nabla f(a)\|}$ which implies that the direction
	of the gradient is the direction of the steepest ascent. Conversely,
	the direction orthogonal to it, is the direction of least change.
	\begin{proposition}
		Let $f \colon U \to \R$ for $U \subset \R^n$ be differetiable in $U$.
		Then if $f$ has a local minimum or maximum at $a \in U$ then:
		\[
			\nabla f(a) = 0
		\]
	\end{proposition}
	\noindent
	This is simply a reiteration of Fermat's theorem for the $n$-dimensional
	case.
	\begin{proposition}
		A linear function $f \colon \R^n \to \R^m$ is differentiable at 
		$a \in U$ if and only if $f_i = f \circ \pi_i$ is differentibale 
		for every $1 \le i \le n$ where $\pi_i$ is the projection function 
		on the $i$th coordinate.
	\end{proposition}
	\begin{proof}
		Suppose that $f_i$ is differentiable for every $1 \le i \le n$ then
		we can define a new function:
		\[
			Tv := \begin{pmatrix}Df_1(a)v \\ Df_1(a)v \\ 
			\vdots \\ Df_m(a)v \end{pmatrix}
		\]
		And see that indeed:
		\[
			\|f(a + h) - f(a) - Th\| \le 
			\sum_{i=1}^{m}|f(a + h) - f(a) - Df_i(a)h| = 
			\sum_{i=1}^{m}|\epsilon_i(h)| = o(h)
		\]
		As wanted. On the other hand, if $f$ is differentiable we can use
		the chain rule to show that each $f_i$ is differentiable with the
		desired derivative.
	\end{proof}
	\begin{definition}
		Let $f \colon \R^n \to \R^m$ such that each of its first-order partial 
		derivatives exists. We define the \textbf{Jacobian} of $f$ to be the 
		matrix:
		\[
			{J_{f}} ={\begin{bmatrix}{\dfrac {\partial {f} }
			{\partial x_{1}}}&\cdots &{\dfrac {\partial {f} }{\partial 
			x_{n}}}\end{bmatrix}}={\begin{bmatrix}\nabla ^{\mathrm {T} }f_{1}\\
			\vdots \\\nabla ^{\mathrm {T} }f_{m}\end{bmatrix}}={\begin{bmatrix}
			{\dfrac {\partial f_{1}}{\partial x_{1}}}&\cdots &{\dfrac {\partial 
			f_{1}}{\partial x_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial 
			f_{m}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{m}}{\partial 
			x_{n}}}\end{bmatrix}}
		\]
	\end{definition}

	\newpage
	
	\section{Least Squares and Gradient Descent}
	
	\newpage
	
	\section{Taylor's Theorem}
	\subsection{Higher Order Partial Derivatives}
	\begin{theorem}
		\tt{Schwart'z Theorem}
		A function $f \colon \Omega \to \R$ defined
		on a set $\Omega \subset \R^n$, then if $p \in \Omega$ is a point
		with some neighbourhood contained in $\Omega$, and $f$ has continuous
		second partial derivatives in that neighbourhood then for all $i,j$
		in $\{1,2,\dots,n\}$,
		\[
			{\frac {\partial ^{2}}{\partial x_{i}\,
			\partial x_{j}}}f({p})={\frac {\partial ^{2}}{\partial x_{j}\,
			\partial x_{i}}}f({p}).
		\]
	\end{theorem}
	
	\subsection{Multi-index Notation}
	An $n$-dimensional multi-index is an $n$-tuple
  \[
    \alpha = (\alpha_{1},\alpha_{2},\ldots,\alpha_{n})
  \]
	of non-negative integers. Define the following operations on some
	multi-indices $\alpha ,\beta \in \N_{0}^{n}$ and a vector
	$x=(x_{1},x_{2},\ldots ,x_{n})\in \mathbb {R} ^{n}$: \\
  \phantom{} \\
	\textbf{Addition and Subtraction}
	\[
		\alpha \pm \beta =
		(\alpha _{1}\pm \beta _{1},\,\alpha _{2}\pm 
		\beta _{2},\ldots ,\,\alpha _{n}\pm \beta _{n})
	\]
	\textbf{Partial Order}
	\[
		\alpha \leq \beta \quad \Leftrightarrow \quad 
		\alpha _{i}\leq \beta _{i}\quad \forall \,i\in \{1,\ldots ,n\}
	\]
	\textbf{Absolute Value}
	\[
		|\alpha |=\alpha _{1}+\alpha _{2}+\cdots +\alpha _{n}
	\]
	\textbf{Factorial}
	\[
		\alpha !=\alpha _{1}!\cdot \alpha _{2}!\cdots \alpha _{n}!
	\]
	\textbf{Binomial Coefficient}
	\[
		{\binom {\alpha}{\beta}} = 
		{\binom {\alpha_{1}}{\beta_{1}}}
		{\binom {\alpha_{2}}{\beta_{2}}}\cdots 
		{\binom {\alpha_{n}}{\beta_{n}}} = 
		{\frac {\alpha!}{\beta! (\alpha-\beta)!}}
	\]
	\textbf{Multinomial Coefficient}
	\[
		{\binom {k}{\alpha}} = 
		{\frac {k!}{\alpha_{1}!\alpha_{2}! \cdots \alpha_{n}!}} = 
		{\frac {k!}{\alpha!}}, 
		\quad k:=|\alpha |\in \mathbb {N}_{0}
	\]
	\textbf{Power}
	\[
		x^{\alpha} = 
		x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}\ldots x_{n}^{\alpha_{n}}
	\]
	\textbf{Higher Order Partial Derivatives}
	\[
		\partial^{\alpha} = 
		\partial_{1}^{\alpha _{1}}
		\partial _{2}^{\alpha _{2}} \ldots 
		\partial _{n}^{\alpha _{n}},
	\]
	\subsection{Taylor's Theorem}
	\tt{Multivariate Version of Taylor's Theorem}
	Let $f \colon \R^n \to \R$ be a $k$-times continuously differentiable 
	function at the point $\boldsymbol{a} \in \R^n$. Then there exist functions 
	$h_\alpha \colon \R^n \to \R$, where $|\alpha| = k$, such that:
	\[
		{\begin{aligned}&f({\boldsymbol {x}})=\sum _{|\alpha |\leq k}{\frac 
		{D^{\alpha }f({\boldsymbol {a}})}{\alpha !}}({\boldsymbol {x}}-
		{\boldsymbol 
		{a}})^{\alpha }+\underbrace{\sum _{|\alpha |=k}h_{\alpha }({\boldsymbol 
		{x}})({\boldsymbol {x}}-{\boldsymbol {a}})^{\alpha }}_{o(\|x-\alpha\|
		^k)},\\&{\mbox{and}}\quad \lim _{{\boldsymbol {x}}\to {\boldsymbol {a}}}
		h_{\alpha }({\boldsymbol {x}})=0.\end{aligned}}
	\]
	
	\newpage
	
	\section{Determining Critical Points}
	Suppose that $f \colon U \to \R$ such that $U \subset \R^n$ is open
	and $a \in U$.
	\begin{definition}
		If exists $r > 0$ such that for any point $x \in B_r(a)$ we have
		$f(a) \le f(x)$ then the point is called a \textbf{local weak minimum}.
	\end{definition}
	\begin{remark}
		If the inequality is strict it is called a \textbf{local strong
		maximum}.
	\end{remark}
	\noindent We define a local weak/strong maximum in the same manner.
	\begin{definition}
		If $f$ is differentiable at $a$ and $\nabla f(a) = 0$ then $a$
		is called a \textbf{critical point} of $f$.
	\end{definition}
	\begin{definition}
		If $a$ is a critical point of $f$ and for any $r > 0$ exist 
		$x,y \in B_r(a)$ such that $f(x) < f(a) < f(y)$ then $a$ is called
		a \textbf{saddle point} of $f$.
	\end{definition}
	\todo[inline]{ADD EXAMPLES HERE USING PGFPLOTS}
	\begin{definition}
		Let $f \in C^2(U)$. We define the \textbf{Hessian} of $f$ at $a$ to be
		the matrix:
		\[
			H_f = H_f(a) = 
			\left[ \frac{\partial^2 f(a)}{\partial x_i \partial x_i} \right]_
			{i,j = 1}^{n}
		\]
	\end{definition}
	Notice that by Clairaut's theorem we have that the Hessian is symmetrical
	and thus it defines a quadratic form as follows:
	\[
		\ip{H_f(a)v}{v} = \sum_{i,j}
		{\frac{\partial^2 f}{\partial x_i \partial x_i}(a)v_i v_j}
	\]
	Using Taylor's theorem and the multi-index notation explained earlier
	we can express the function as such:
	\[
		f(x + h) = f(x) + \sum_{i=1}^{n}{D_i f(x) h_i} + 
		\sum_{|\alpha|=2}{\frac{D^\alpha f(x) h^\alpha}{\alpha!}} + o(\|h\|^2)
	\]
	We can notice that:
	\[
		|\alpha| = 2 \Rightarrow \alpha_1 + \cdots + \alpha_n = 2
	\]
	Which means that either way $\alpha! = 2$, so after some algebraic
	manipulation:
	\[
		f(x + h) = f(x) + \ip{\nabla f(x)}{h} + \frac{1}{2}\ip{H_f(x)h}{h}
		+ o(\|h\|^2)
	\]
	Now if assume that $x$ is a critical point of $f$, we can see that magic
	happens. We have $\nabla f(x) = 0$ and then:
	\[
		f(x + h) - f(x) = \ip{0}{h} + \frac{1}{2}\ip{H_f(x)h}{h}
		+ o(\|h\|^2) \approx \frac{1}{2}\ip{H_f(x)h}{h}
	\]
	Since $H_f(x)$ is symmetrical it is similar to a diagonal matrix
	which we may denote $A = \mathrm{diag}(\lambda_1,\dots,\lambda_n)$,
	and under the coordinates system corresponding to the eigenvectors
	of the Hessian we get that:
	\[
		f(x + h) - f(x) \approx \frac{1}{2}\ip{H_f(x)h}{h} = 
		\sum_{i=1}^{n}{\lambda_i h_i^2}
	\]
	And in case $n = 2$ we have more simply:
	\[
		f(x + (a,b)) - f(x) \approx \lambda_1 a^2 + \lambda_2 b^2
	\]
	And we can use this formula to determine the type of critical points.
	
	\newpage
	
	Suppose that $a \in U$ is a critical point of $f$, then the following
	hold:
	\begin{enumerate}
		\item If $H_f(a)$ is \textbf{positive definite} then $a$ is a 
		\textbf{local strong minimum.}
		\item If $H_f(a)$ is \textbf{negative definite} then $a$ is a 
		\textbf{local strong maximum.}
		\item If $H_f(a)$ is \textbf{indefinite} then $a$ is a 
		\textbf{saddle point.}
	\end{enumerate}
	In other cases we can't tell anything about the point and it might be
	of any type.
	\todo[inline]{ADD EXAMPLES}

	\newpage
	
	\section{The Inverse Function Theorem}
	A set $C \subset \R^n$ is called \textbf{convex} if and only if:
	\[
		\forall x,y \in C \quad \forall t \in [0,1] \quad
		tx + (1-t)y = y + t(x-y) \in C
	\]
	Intuitively, we say that $C$ is convex if for every two points $a,b \in C$
	the interval connecting the points is a subset of $C$.
	\begin{proposition}
		Let $U \subset \R^n$ be convex and open, let $f \colon U \to \R^n$
		be differentiable. If $f'(x)$ is bounded by $M$ then,
		\[
			 \forall x,y \in U \quad \|f(x) - f(y)\| \le M\|x - y\|
		\]
	\end{proposition}
	\begin{proof}
		If $f(x) = f(y)$ then we are trivially finished, otherwise we can
		denote $v = \frac{f(x) - f(y)}{\|f(x) - f(y)\|}$ and the following
		functions:
		\begin{align*}
			&g \colon \R^n \to \R \quad \quad g(u) = \ip{v}{u} \\
			&h \colon [0,1] \to U \quad h(t) = y + t(x-y)
		\end{align*}
		Differentiating gives:
		\begin{align*}
			&Dg = v \\
			&Dh = x-y
		\end{align*}
	We can now define the function:
	\[
		\varphi = g \circ f \circ h
	\]
	And apply Lagrange's theorem to get that exists $c \in (0,1)$ such that:
	\[
		\varphi'(c) = \frac{\varphi(1) - \varphi(0)}{1 - 0} = 
		\ip{v}{f(x)} - \ip{v}{f(y)} = \|f(x) - f(y)\|
	\]
	On the other hand we have:
	\[
		|\varphi'(c)| = \|g'(f \circ h)(f \circ h)'(h)'\| \le 
		\|v\| \cdot M \cdot \|x - y\|
	\]
	But we know that $\|v\| = 1$ so we get:
	\[
		\|f(x) - f(y)\| \le M\|x - y\|
	\]
	As wanted.
	\end{proof}
	\noindent Try showing this still stands for the $|\cdot|_\infty$ norm.
	\begin{corollary}
		If $f \in C^1$ then $f$ is Lifschitz continuous on any compact
		subset of its domain.
	\end{corollary}
	\begin{theorem}
	\tt{Inverse Function Theorem}
	Let $U \subset \R^n$ be open,
	$f \in C^1(U,\R^n)$, and $a \in U$. If $f'(a)$ is invertible then
	exist open sets $a \in V \subset U$ and $f(a) \in W$ such that
	$f \colon V \to W$ is a bijection and $f^{-1}$ is also continuously 
	differetiable and:
	\[
		(f^{-1})'(f(x)) = [f'(x)]^{-1}
	\]
	\end{theorem}
	\todo[inline]{ADD PROOF}
	
	
	\newpage
	
  \section{Newton\textendash{}Raphson Method}
	
	\newpage
	
	\section{The Open Mapping Theorem}
	\begin{definition}
		Suppose $U \subset \R^n$. A function $f \in C^1(U, \R^m)$ is called 
		\textbf{regular} in a point $a \in U$ if $\mathrm{rank}J_f(a) = m$.
		The function is called regular in $U$ if it is regular for any
		$a \in U$.
	\end{definition}
	\begin{definition}
		A mapping $f \colon U \to V$ is called \textbf{open} if it send
		any open set $W$ to an open set. That is for any open set 
		$W \subset U$ then $f(W) \subset V$ is also open.
	\end{definition}
	\begin{theorem}
		\tt{The Open Mapping Theorem}
		Let $U \subset \R^n$ be an open set, and let $f \in C^1(U,\R^m)$ be 
		regular in $U$. Then $f$ is an open mapping.
	\end{theorem}
	\begin{proof}
		In the case of $m = n$ the theorem follows directly from the inverse
		function theorem. Because $U$ is open, and because we know that
		for any $a \in U$ that $J_f(a)$ is invertible, we get from the
		inverse function theorem that exist open sets $a \in V_a \subset U$ 
		and $f(a) \in W_a = f(V_a)$. We then have clearly that:
		\[
			f(U) = \bigcup_{a \in U} W_a
		\]
		Since $W_a$ are open for any $a \in U$ we get that $f(U)$ is open
		as a union of open sets. For any open set $V \subset U$ we can
		use this exact proof using $f\vert_V$ instead. \\
		Otherwise we must have $m < n$ because if $n < m$ the rank of the
		Jacobian can't be $m$. Considering our case, the rank of the Jacaobian
		is $m$ which means it has $m$ linearly independent columns. Without
		lose of generality we assume they are the first $m$ columns and 
		denote:
		\[
			\frac{\partial(f_1,\dots,f_m)}{\partial(x_1,\dots,x_m)} = 
			\left[\frac{\partial f_i}{\partial x_j}\right]_{(i,j)=1}^{m}
		\]
		We now define a new transformation as such:
		\begin{align*}
			&F \colon U \to \R^n \\
			&F(x) = (f(x),x_{m+1},\dots,x_{n})
		\end{align*}
		We see that the Jacobian of $F$ is:
		\[
			J_F = \begin{pmatrix}
	\cfrac{\partial(f_1,\dots,f_m)}{\partial(x_1,\dots,x_m)} & \star \\
	0 & I_{(n-m) \times (n-m)}\\
				\end{pmatrix}
		\]
		We notice that $F$ satisfies the conditions for the inverse function
		theorem and thus it is an open mapping. We also notice that for
		every open set $V \subset U$ that:
		\[
			f(V) = \pi(F(V))
		\]
		Where $\pi$ is the projection 
		$\pi \colon (x_1,\dots,x_n) \mapsto (x_1,\dots,x_m)$. Since
		it is clear that $\pi$ is an open mapping we have that $f$
		is also an open mapping as a composition of two open mappings as
		wanted.
	\end{proof}
	
	\newpage
	
	\section{Constrained Optimization}
	\begin{proposition}
		Let $g \in C^1(U,\R^m)$ be regular,
		$M = \{x \in U \mid g(x) = 0\}$, and $f \in C(U)$. If $a \in M$ and
		$f(a) \le f(x)$ for any $x \in M$ then:
		\[
			\nabla f(a) \in \mathrm{span}\{\nabla g_1(a),\dots,\nabla g_m(a)\}
		\]
	\end{proposition}
	\noindent
	To actually find the minimum we solve the system of equations:
	\[
		\begin{cases}
			\nabla f(a) = \sum_{i = 1}^{m}{\lambda_i \nabla g_i(a)} \\
			g(a) = 0
		\end{cases}
	\]
	Notice that the first equation gives $n$ equations in $n + m$ variables
	that are $a_1,\dots,a_n,\lambda_1,\dots,\lambda_m$ and the second
	equation gives $m$ equations in $n$ variables, thus we have $n + m$
	equation in $n + m$ variables.
	\begin{remark}
		The variables $\lambda_i$ for $1 \le i \le m$ are called
		\textbf{Lagrange multipliers}.
	\end{remark}
	Here's a simple example to how we can use Lagrange's multiplies to find
	the distance of a plane from the origin. The function we want to minimize
	is:
	\[
		f(x,y,z) = \|(x,y,z)\|_2 = \sqrt{x^2 + y^2 + z^2}
	\]
	But this is actually equivalent to finiding the minimum of:
	\[
		f(x,y,z) = x^2 + y^2 + z^2
	\]
	Under the constraints of a plane:
	\[
		g(x,y,z) = ax + by + cz - d = 0
	\]
	We notice that $g$ is indeed regular. To find the minimum we will solve
	the system of equations:
	\[
		\begin{cases}
			\nabla f(x,y,z) = \sum_{i = 1}^{m}{\lambda_i \nabla g_i(x,y,z)} \\
			g(x,y,z) = 0
		\end{cases}
	\]
	Since $g$ is a function to $\R$ we only have one Lagrange multiplier.
	We can calculate the gradients of the functions:
	\begin{align*}
		\nabla f(x,y,z) &= (2x,2y,2z) \\
		\nabla g(x,y,z) &= (a,b,c)
	\end{align*}
	And put them in the system to get:
	\[
		(2x,2y,2z) = \lambda (a,b,c) \quad\text{thus}\quad
		(x,y,z) = \frac{\lambda}{2} (a,b,c)
	\]
	from the first equation. Putting that in the second equation gives:
	\[
		g\left(\frac{\lambda}{2} (a,b,c)\right) = 
		\frac{\lambda}{2}(a^2 + b^2 + c^2) - d = 0
	\]
	Finally we get that the Lagrange multiplier is:
	\[
		\lambda = \frac{2d}{a^2 + b^2 + c^2}
	\]
	And the minimum point is:
	\[
		(x,y,z) = \frac{d}{a^2 + b^2 + c^2} (a,b,c)
	\]
	And the minimal distance of the plane from the origin is:
	\[
		\|(x,y,z)\| = \left\|\frac{d}{a^2 + b^2 + c^2} (a,b,c)\right\| = 
		\frac{d}{\|(a,b,c)\|}
	\]
	
	\newpage
	
	In fact what we have shown so far is not a complete proof because
	the theorem only implies that at a minimum point the equations hold
	but it doensn't necessarily mean that the point we found is a minimum
	point. To complete the proof we can choose an arbitrary point on the
	plane $P_0 = (x_0,y_0,z_0)$, and denote $R = 2\|P_0\|$, and consider
	the set:
	\[
		S = \{x \in \R^3 \mid g(x) = 0\} \cap \overline{B_R(0)}
	\]
	This set is clearly compact and since $P_0 \in S$ we know that it's not 
	empty and thus we know that $f$ has a minimum in that set, and moreover,
	we know a priori that this minimum must be the global minimum of the
	function. Since we only found one point satisfying the equations given
	by the theorem it must be that minimum point and now the proof is 
	complete.
	
	\begin{proof}
		To prove the theorem we will prove the contrapositive. Suppose that:
		\[
			\nabla f(a) \notin 
			\mathrm{span}\{\nabla g_1(a),\dots,\nabla g_m(a)\}
		\]
		We need to show that $a$ is not a minimum point. Define the function:
		\begin{align*}
			&F \colon U \to \R^{m+1} \\
			&F(x) = (f(x),g(x))
		\end{align*}
		The Jacobian will be:
		\[
		J_F(a) = 
			\begin{bmatrix}
				\nabla f(x)\\
				\nabla_1(x)\\
				\vdots\\
				\nabla_m(x)\\
			\end{bmatrix}
		\]
		Because we assumed $\nabla f(a)$ in linearly independent from the
		rest of the gradients at $a$ the rank of the Jacobian at $a$ must
		be $m + 1$ and since the rank function is continuous we know that
		exists a neighborhood $U$ of $a$ where the Jacobian rank is $m + 1$
		and thus we can allpy the open mapping theorem and get that $F$
		is an open mapping in that neighborhood. We know that 
		$F(a) = (f(a), 0)$ is in the open set $F(U)$ and also that for a small
		enough epsilon that $(f(a) - \varepsilon, 0) \in F(U)$ and thus
		exists $x \in U$ such that 
		$F(x) = (f(a) - \varepsilon), 0) = (f(x), g(x))$ which means that:
		\begin{enumerate}
			\item $g(x) = 0$
			\item $f(x) < f(a)$
		\end{enumerate}
		It follows that $x \in M$ and $f(x) < f(a)$ and thus $a$ is not
		a minimum point in $M$.
	\end{proof}
	
	\newpage
	
	
	\section{The Implicit Function Theorem}
	In mathematics we have so far talked about explicit function - functions
	that take the elements of one set and transform them into an element
	of the other. In contrast, we can consider a new way of looking at
	functions using implicit functions. First, we will consider 
	implicit equations.
	\begin{definition}
		An \textbf{implicit equation} is a relation of the form 
		$R(x_1,\dots,x_n) = 0$ where $R$ is a multivariable function.
		A vector $x \in \R^n$ that satisfies $R(x) = 0$ is called a
		\textbf{solution}.
	\end{definition}
	We say that the implicit equation $F(x,y)$ where 
	$(x,y) \in U \subset \R^n \times \R^m$ defines $y$ as an implicit 
	function of $x$ if exist $V_1 \times V_2 \subset U$ such that 
	for any $x \in V_1$ exists a unique $y \in V_2$ such that $F(x,y) = 0$.
	Intuitively the function maps each $x \in V_1$ to its corresponding
	unique $y$.
	\begin{theorem}
		\tt{The Open Mapping Theorem}
		Let $U \subset \R^n \times \R^m$ be open, $f \in C^1(U,\R^m)$, $(a,b)$ 
		a solution of $f(x,y) = 0$ and also assume that $\mathrm{det}
		\frac{\partial(f_1,\dots,f_m)}{(x_1,\dots,x_m)}\vert_{(a,b)} \neq 0$.
		Then exists a neighborhood $(a,b) \in V$ and a function $g \in C^1$ 
		defined around $a$ such that:
		\[
			\forall (x,y) \in V \quad f(x,y) \iff y = g(x)
		\]
		And $g(a) = b$ and $f(x,g(x)) = 0$ for every $x$ near $a$.
	\end{theorem}
	\begin{theorem}
		\tt{Implicit Function Differentiation}
		Let $U \subset \R^n \times \R^m$ be an open set, $f \in C^1(U,\R^m)$, 
		$(a,b)$ a solution of $f(x,y) = 0$, and also assume that $\mathrm{det}
		\frac{\partial(f_1,\dots,f_m)}{(x_1,\dots,x_m)}\vert_{(a,b)} \neq 0$.
		Then the derivative of the function $g$ from the implicit function
		theorem is:
		\[
			g'(a) = -[D_y f(a,b)]^{-1} D_x f(a,b)
		\]
	\end{theorem}
	Notice that this thoerem is very useful because it allows us to find
	$g'(a)$ without knowing what is $g(a)$ itself.
	\todo[inline]{ADD PROOFS}
	
	\newpage
	
	\section{Finding Roots of Polynomials}
	So it turns out that given a polynomial $p(x) = \sum_{i=0}^{n}{a_n x^n}$
	then its root are continuously dependent on the coefficients.
	\begin{definition}
		Let $p(x) = \sum_{i=0}^{n}{a_n x^n}$ be a polynomial. A root of the
		polynomial with multiplicity of $1$ is called a \textbf{simple root}.
	\end{definition}
	We want to show that if $x_0$ is a simple root of a polynomial 
	$P_a(x) = \sum_{i=0}^{n}{a_n x^n}$ such that $a = (a_0,\dots,a_n)$
	then exists a neighborhood $V$ of $a$ in $\R^{n+1}$ such that one of the
	roots of the polynomial:
	\[
		P_b(x) = \sum_{i=0}^{n}{b_n x^n}
	\]
	For any $b \in V$ are given as a function $g \in C^1(\R^{n+1})$.
	\begin{proof}
		Define the function:
		\begin{align*}
			&f \colon \R^{n + 2} \to \R \\
			&f(a_0,\dots,a_n,x) = P_a(x)
		\end{align*}
		We notice that the implicit equation $f(a,x) = 0$ gives all the roots
		of the polynomial $P_a(x)$. Denote $x_0$ a simple root of $P_a(x)$
		where $a$ is the coefficients vector of the polynomial. We
		can calculate that:
		\[
			f(a,x_0) = 0
		\]
		And since $x_0$ is a simple root we also know that:
		\[
			\frac{df}{dx}\biggr\vert_{(a,x_0)} = 
			\frac{dP_{a}}{dx}\biggr\vert_{x_0} \neq 0 
		\]
		Therefore we satisfied the conditions for the implicit function
		theorem and can conclude that exists a neighborhood $V$ of $(a,x_0)$
		and a function $g \in C^1$ defined around $a$ such that:
		\[
			\forall (a,x) \in V \quad P_a(x) = 0 \iff x = g(a)
		\]
	\end{proof}
	This theorem of great importance because when using computers and 
	numerical methods to find roots using approximate coefficients we want
	to know that the roots we find are a good approximation of the real
	roots we need to find and this theorem shows exactly that.
	
	\newpage
	
	\section{Manifolds}
	\begin{definition}
		Let $k,n \in \N$ such that $k \le n$. A subset $M \subset \R^n$
		is called a \textbf{$C^1$ manifold of dimension \textbf{$k$}}
		if for every $a \in M$ exist open sets $U,V \subset \R^n$ such that
		$a \in U$ and $V \cap \R^k \times \{0_{n-k}\} \neq \emptyset$ and 
		exists a regular function $f \in C^1(U,V)$ such that:
		\[
			f(M \cap U) = (V \cap \R^k \times \{0_{n-k}\}) = 
			\{x \in V \mid x_{k+1} = \cdots = x_n = 0\}
		\]
	\end{definition}
	Recall that the definition is regular is that in any point in the domain
	the derivative at that point is a surjective linear transformation - or
	in other words - the rank of the Jacobian is equal to its row number.
	Because $f$ is a transformation between two open sets of the same
	dimension we know that $f$ is regular if and only if its derivative
	is invertible at any point.
	\begin{definition}
		A function $f \in C^1$ that is invertible and regular is called
		a \textbf{diffeomorphism}. From the inverse function theorem
		every diffeomorphism's inverse function is a diffeomorphism in itself.
	\end{definition}
	\begin{remark}
		A $1$-dimensional manifold is sometimes called a 
		\textbf{differential curve} and a $2$-dimensional manifold is 
		sometimes called a \textbf{differential surface}.
	\end{remark}
	The title of this section is indeed ``manifolds'' but in fact we
	are only talking about \textbf{embedded manifolds}. That is to say,
	we are not talking about manifolds in the abstract sense at all,
	only about those who are specifically embedded in the Euclidean space.
	\begin{proposition}
		The following conditions are equivalent:
		\begin{enumerate}
			\item $M$ is a $C^1$ manifold of dimension $k$
			\item For every $a \in M$ exists a neighborhood $U_a$ and a 
			regular function $g \in C^1(U,\R^{n-k})$ such that:
			\[
				M \cap U = \{x \in U \mid g(x) = 0\}
			\]
			\item Up to permutation of the variables, for every $a \in M$
			exists a neighborhood $a \in V \times W$ such that 
			$V \subset \R^k$ and $W \subset \R^{n-k}$ and exists
			$h \in C^1(V, \R^{n-k})$ such that:
			\[
				M \cap (V \times W) = \mathrm{graph}(h) = 
				\{(x,h(x)) \mid x \in V\}
			\]
			\item For each $a \in M$ exists a neighborhood $U$, an open
			set $V \subset \R^k$ and an injective function $H \in C^1(V,\R^n)$
			such that:
			\begin{enumerate}
			\item $\mathrm{rank} DH = k$
			\item $M \cap U = H(V)$
			\item The function $H^{-1} \colon H(V) \to V$ is continuous
			according to the topology on $H(V)$ induced by $\R^n$.
			\end{enumerate}
		\end{enumerate}
	\end{proposition}
	\begin{remark}
		The function $H$ from 4 is called a parametrization of $M \cap U$.
	\end{remark}
	
	\begin{example}
		Let $S^n = \partial \mathbb{B}_{n+1} = \{x \mid \|x\| = 1\} \subset 
		\R^{n+1}$ be the $n$-dimensional sphere. We will show that this
		sphere is a $n$-dimensional manifold.
	\end{example}
	\noindent
	\emph{According to 2} \\
	For any $a \in M$ we can choose the open set 
	$U = \R^{n+1} \setminus \{0\}$ such that $x \in U$. Define the function:
	\begin{align*}
		&g \colon U \to \R \\
		&g(x) = \left(\sum_{i=1}^{n+1}{x_i^2}\right) - 1
	\end{align*}
	We see that:
	\[
		\nabla g = 2(x_1,\dots,x_{n+1})
	\]
	So $g$ is regular and continuously differentiable in $U$. Finally we
	check that:
	\[
		S^n \cap U = S^n = \{x \in U \mid g(x) = 1\}
	\]
	So indeed this verifies that $S^n$ is an $n$-dimensional manifold. \\
	\noindent
	\emph{According to 3} \\
	Let $a \in M$, without loss of generality and allowing permutation
	of variables we can assume that $a_{n+1} < 0$. Now choose 
	$V = \mathbb{B}_n$ and $W = (-\infty,0)$. Define:
	\begin{align*}
		&h \colon V \to \R \\
		&h(v_1,\dots,v_n) = - \sqrt{1 - \sum_{i=1}^{n}{v_i^2}}
	\end{align*}
	We can verify that indeed:
	\[
		S^n \cap (\mathbb{B}_n \times (-\infty,0)) = \{(v,h(v)) \mid v \in V\}
	\]
	\emph{According to 4} \\
	We will show only the case of $S^n = S^2$. Suppose we want to find
	a parametrization at the point $(1,0,0)$. We define the sets:
	\begin{align*}
		V &= (0,\pi) \times \left(-\frac{\pi}{2}, \frac{\pi}{2}\right) \\
		U &= \{(x,y,z) \mid x > 0\}
	\end{align*}
	And the function:
	\[
		H(\theta, \phi) = 
		(\sin(\theta)\cos(\phi),\sin(\theta)\sin(\phi),\cos(\theta))
	\]
	We can see that $H$ is indeed a continuously differentiable continuous
	injection. We see that:
	\[
		DH = J_H = \begin{pmatrix}
					\cos(\theta)\cos(\phi) & -\sin(\theta)\sin(\phi)\\
					\cos(\theta)\sin(\phi) & \sin(\theta)\cos(\phi)\\
					-\sin(\theta) & 0\\
					\end{pmatrix}
	\]
	In our case $k = 2$, so seeing that the determinant of the upper
	is $\sin(\theta)\cos(\theta)$ which is $0$ only when 
	$\theta = \frac{\pi}{2}$ but then the matrix is:
	\[
		\begin{pmatrix}
		0 & \sin(\phi)\\
		0 & \cos(\phi)\\
		-1 & 0\\
		\end{pmatrix}
	\]
	Which is a matrix of rank $2$ so we conclude that $\mathrm{rank}DH = 2$
	for every value pair $(\theta,\phi)$ in $V$. Conditions 
	$\langle b \rangle$ and $\langle c \rangle$ also hold after some
	algebraic manipulation.
	\begin{remark}
		Every open set $U \subset \R^n$ is a manifold of dimension $n$
		and every vector space $V \subset \R^n$ is a manifold of dimension 
		$\dim(V)$.
	\end{remark}
	\todo[inline]{ADD PROOF OF THE EQUIVALENCIES}
	
	\begin{definition}
		Let $M \subset \R^n$ be a manifold of dimension $k$. A function
		$g \colon M \to \R^m$ is called \textbf{continuously differentiable}
		if for every $a \in M$ and every parametrization $(H,V)$ in $a$
		we have:
		\[
			g \circ H \in C^1(V, \R^m)
		\]
	\end{definition}
	\noindent
	For practice, show that the definition is not dependent on the choice of 
	the coordiante system. Also, show that this definition is equivalent to 
	exists a neighborhood $a \in U$ and a function $G \in C^1(U,\R^m)$ such 
	that  $G \vert_{M \cap U} = g \vert_{M \cap U}$ where $g$ is the $C^1$ 
	function.
	
	\newpage
	
	\section{The Tangent Space}
	\begin{definition}
		A continuous function $f \colon [a,b] \to \R^n$ is called a
		\textbf{continuous path}. If $f$ is also differentiable we say
		it is a \textbf{continuously differentiable path}. The ``$1$-
		dimensional'' set $f([a,b])$ is called a \textbf{curve}.
	\end{definition}
	\todo[inline]{ADD INTUITION FOR THE TANGENT SPACE}
	\begin{definition}
		For each point $p \in \R^n$ we define the \textbf{tangent space}
		to $\R^n$ in $p$ as:
		\[
			T_p(\R^n) = \{v_p = (p,v) \mid v \in \R^n\}
		\]
	\end{definition}
	In manifolds we have the following definition
	\begin{definition}
		Let $M$ be a $C^1$ manifold of dimension $k$, and let $p \in M$ 
		be a point on the manifold. The \textbf{tangent space} to $M$ in 
		point $p$ is defined as:
		\[
			\left\{
				p + \gamma'(t_0) \biggr\vert 
				\substack{
				C^1 \ni \gamma \colon (a,b) \to M \\
				t_0 \in (a,b) \quad \gamma(t_0) = p}
			\right\}
		\]
		And we also denote:
		\[
			T_p(M) = \left\{
				\gamma'(t_0) \biggr\vert 
				\substack{
				C^1 \ni \gamma \colon (a,b) \to M \\
				t_0 \in (a,b) \quad \gamma(t_0) = p}
			\right\}
		\]
	\end{definition}
	\begin{remark}
		Let $H \colon V \subset \R^k \to \R^n$ be a parametrization
		of a manifold $M$ around $p$, and $q \in V$ a point such that
		$H(q) = p$. Then $T_p(M) = [\mathrm{Im}(DH(q))]_p$.
	\end{remark}
	\begin{example}
		Consider $S^1$ and the parametrization:
		\begin{align*}
			&H \colon \left(\frac{\pi}{2}, \frac{3\pi}{2}\right) \\
			&H(t) = (\cos(t), \sin(t))
		\end{align*}
		Choose $p = (-1,0) = H(\pi)$. We see that:
		\[
			DH(\pi) = 
			\begin{pmatrix}
				-\sin(t)\\
				\cos(t)\\
			\end{pmatrix}_{t=\pi} = 
			\begin{pmatrix}
				0\\
				-1\\
			\end{pmatrix} \colon \R \to \R^2
		\]
		We notice that indeed:
		\begin{align*}
			&\mathrm{Im}(DH(\pi)) = \mathrm{Sp}
			\left\{\begin{pmatrix}
				0\\
				-1\\
			\end{pmatrix}\right\} \\
			\Rightarrow \,&T_p(M) =
			\left\{p + a\begin{pmatrix}
				0\\
				-1\\
			\end{pmatrix} \biggr\vert a \in \R \right\}
		\end{align*}
	\end{example}
	\todo[inline]{MORE DEFINITIONS \\}
	\begin{definition}
		Let $N \subset \R^3$ be a smooth surface. A vector 
		$0 \neq v \in \R^3$ is said to be \textbf{normal} to $M$ at a 
		point $p \in M$ if it is orthogonal to $T_p(M)$.
	\end{definition}
	\begin{remark}
		If $M$ is equal to the set of roots of a regular function $g$
		in a neighborhood of $a$ then a vector normal to $M$ must be 
		co-linear with $\nabla g(p)$. 
		\todo{add later} That is because
	\end{remark}
	
	\newpage
	
	\section{Manifolds With a Boundary}
	Defining manifolds is very convenient, but many intresting sets
	we want to analyze are not manifolds like any closed interval or
	half a sphere. That is why in this section we introduce a new object
	that is almost like a manifold, but slightly different.
	\begin{definition}
		The \textbf{half $k$-dimensional space} is defined as:
		\[
			\mathcal{H}_k = \{x \in \R^n \mid x_n \geq 0\}
		\]
	\end{definition}
	\begin{definition}
		A subset $M \subset \R^n$ is called a $C^1$ \textbf{manifold 
		with boundary} of dimension $k$ if for every $a \in M$ exists
		a neighborhood $a \in U$, an open set $V \subset \R^k$ and
		an injection $H \in C^1(V, \R^n)$ such that:
		\begin{enumerate}
			\item $\mathrm{rank}DH = k$.
			\item $M \cap U = H(V \cap \mathcal{H}_k)$.
			\item $H^{-1} \colon M \cap U \to V \cap \mathcal{H}_k$
			is continuous with respect to the topology induced by $\R^n$.
		\end{enumerate}
		The function $H$ is called a \textbf{parametrization} of $M$ at
		$a$.
	\end{definition}
	\begin{definition}
		Let $M \subset \R^n$ be a manifold with boundary with dimension
		$k$. A point $a \in M$ is called a \textbf{boundary point}
		if $a = H(b)$ where $b \in \partial \mathcal{H}_k$. We denote
		the set of the boundary points of $M$ as $\partial M$. The
		rest of the points are called \textbf{inner points} and we denote
		them as $\mathrm{int}(M)$.
	\end{definition}
	\begin{remark}
		\underline{Watch out!} the topological definition of a boundary
		is different from the definition of a boundary for a manifold!
	\end{remark}
	For practice here are some exercises:
	\begin{enumerate}
		\item Prove that if $V \subset \R^k$ is an open set such that
		$\mathcal{H}_k \cap V \neq \emptyset$ and if 
		$H \in C^1(V \cap \mathcal{H}_k), \R^n)$ then exists a function
		$\tilde{H} \in C^1(V, \R^n)$ such that 
		$\tilde{H}\vert_{V \cap \mathcal{H}_k)} = H$
		\item Find explicit formulas for a normal vector and a tangent
		space. Find examples of manifolds with and without boundaries.
	\end{enumerate}
	
	\newpage
	
	\section{Length of a Path}
	\begin{definition}
		Let $f \colon [a,b] \to \R^n$ be a continuous path, and let
		$P = (t_0 = a < \cdots < t_k = b)$. Define:
		\[
			l(f,P) = \sum_{i=1}^{k}{\|f(t_i) - f(t_{i-1})\|}
		\]
		And also define:
		\[
			l(f) = \sup_{p}{l(f,p)}
		\]
		If $l(f) < \infty$ we say that $f$ has a \textbf{length} of
		$l(f)$.
	\end{definition}
	\begin{proposition}
		If $f \in C^1([a,b], \R^n)$ then the path $f$ has length.
		Moreover, for every partition $P$ we have 
		$l(f,P) \le \int_{a}^{b}{\|f'(t)\|\,dt}$, and 
		$l(f) = \int_{a}^{b}{\|f'(t)\|\,dt}$.
	\end{proposition}
	\todo[inline]{ADD PROOF}
	\begin{definition}
		A \textbf{simple smooth curve} is a set $C \subset \R^n$ that
		is the image of a path $f \in C^1([a,b],\R^n)$. That satisfies:
		\begin{enumerate}
			\item The path $f$ is an injection on $(a,b)$.
			\item For every $t \in [a,b]$ we have $f'(t) \neq 0$.
		\end{enumerate}
	\end{definition}
	Show that if $f$ is a injective path then the curve it defines is
	a $C^1$ manifold.
	\begin{definition}
		A curve is called \textbf{directed} if we differentiate between
		its beginning and end.
	\end{definition}
	\begin{definition}
		A curve is called \textbf{closed} or a \textbf{loop} if it
		satisfies $f(a) = f(b)$.
	\end{definition}
	\begin{definition}
		We define the \textbf{length} of a curve to be the length
		of the path it's defined by.
	\end{definition}
	Prove that definition $14.5$ is well defined.
	\begin{definition}
		A parametrization of a curve $g$ such that $\|g'(s)\| = 1$
		is called an \textbf{arc length paramtrization}.
	\end{definition}
	\todo{Some stuff is missing}
	
	\newpage
	
	\section{Line Integrals}
	\todo[inline]{ADD PRACTICE FOR ARC LENGTH PARAMETRIZATION}
	
	\begin{definition}
		Let $C \subset \R^n$ be a smooth simple curve with 
		parametrization $C^1 \ni f \colon [a,b] \to \R^n$. If $\rho$ is
		a continuous function on $C$ then we define the 
		\textbf{scalar integral} of $\rho$ on $C$ or \textbf{type one
		line integral} as:
		\[
			\int_{C}{\rho \,ds} = \int_{a}^{b}{\rho(f(t))\|f'(t)\|\,dt}
		\]
	\end{definition}
	Notice that when $p(x) = 1$ we get the length of the curve. We can
	use this type of integral when we want to sum values over that
	curve. For example, a wire can actually be parameterized using
	a function that gives us its concentration of mass at any given
	point and then the integral would give the total mass of the wire.
	\begin{definition}
		Let $S \subset \R^n$. A function 
		$f \colon S \to \cup_{p \in S}{T_p(\R^n)}$ is called a
		\textbf{vector field} if for all $p \in S$ we have
		$f(p) \in T_p(\R^n)$.
	\end{definition}
	A similar definition exists for smooth manifolds instead of $\R^n$ 
	but there's no reason to get into it right now.
	\begin{definition}
		Let $f \colon [a,b] \to \R^n$ be a continuously differentiable
		parametrization of a directed, smooth, simple curve $C$.
		Let $f \colon C \to \R^n$ be a continuous vector field. The
		line integral of $F$ over $C$ is called a \textbf{type two
		line integral} and is defined to be:
		\[
			\int_{C}{\vec{F} \cdot d\vec{r}} = 
			\int_{C}{\sum_{i=1}^{n}{F_i}\,dx_i} :=
			\int_{a}^{b}{F(f(t)) \cdot f'(t) \,dt}
		\]
	\end{definition}
	
	\newpage
	
	\section{Integration}
	\begin{definition}
		A \textbf{box} in $\R^n$ is a set $R \in \R^n$ such that:
		\[
			R = \prod_{i=1}^{k}{[a_i,b_i]}
		\]
		For real numbers $a_i \le b_i$. If exists $i$ such that $a_i=b_i$
		then we say that the box is \textbf{degenerate}.
	\end{definition}
	We define the volume of a box to be:
	\[
		\mathrm{Vol}(A) = \prod_{i=1}^{k}{(b_i-a_i)}
	\]
	A partition of an $n$-dimensional box is similar to the partitions
	of a $2$-dimensional box and thus its definition is omitted.
	If $P$ is a partition of a box $R$ we denote $B \sim Q$ if
	$B$ is a subbox of the partition. Since the rest of the basic
	definitions for a Darboux or Riemann integrals are very similar
	to the second dimensional case, we will only see the way
	to denote what's necessary. Let $f$ be a bounded function on
	a box $R$. We denote the upper and lower Darboux sums with
	regards to a partition $P$ of $R$ as such:
	\[
		U(f,P) = \sum_{B \sim R}{M_Q \cdot \mathrm{Vol}(Q)}
	\]
	Where $M_Q = \sup\{f(x) \mid x \in Q\}$ as usual. We say
	that $f$ is Darboux integrable if:
	\[
		\inf U(f,P) = \sup L(f,P) = I
	\]
	We denote the integral of $f$ over the box as:
	\[
		I = \int_{R}{f\,dV} = 
		\int_{R}{f(x)\,dx} = \int_{R}{f}
	\]
	Most theorems from analysis $2$ also apply here.
	\begin{definition}
		Fubini's theorem - Let 
		$R = [a_1,b_1] \times \cdots \times [a_n,b_n]$ and suppose
		all the functions:
		\begin{align*}
			F_1(x_1,\dots,x_{n-1}) &= \int_{a_n}^{b_n}{f(x)\,dx_n} \\
			F_2(x_1,\dots,x_{n-2}) 
			&= \int_{a_{n-1}}^{b_{n-1}}{f(x)\,dx_{n-1}}
			\int_{a_n}^{b_n}{f(x)\,dx_n}\\
			&\vdots \\
			F_n \equiv I &= \int_{a_{1}}^{b_{1}}{f(x)\,dx_{1}} \cdots
			\int_{a_{n-1}}^{b_{n-1}}{f(x)\,dx_{n-1}}
			\int_{a_n}^{b_n}{f(x)\,dx_n} \\
		\end{align*}
		Are integrable then $I = \int_{R}{f\,dV}$.
	\end{definition}
	To calculate the volume of a space $\Omega$ we can calculate
	the integral $\int_{\Omega}{f\,dV}$ for the function $f \equiv 1$.
	For a simple space in $\R^3$ we can use Fubini's theorem and get:
	\[
		I = \iiint_{\Omega}{f(x,y,z)\,dxdydz}
		  = \int_{a}^{b}{\,dx}
		  	\int_{y_1(x)}^{y_2(x)}{\,dy}
		  	\int_{z_1(x)}^{z_2(x)}{f(x,y,z)\,dz}
	\]
	For the relevant functions. \hfill
	\todo{Add an example}
	
	\newpage
	
	\section{Zero Volume}
	\begin{remark}
		If $f \colon R \to \R^n$ is a continuous function on $R$
		then it is integrable.
	\end{remark}
	\begin{definition}
		A set $A \subset \R^n$ is said to have \textbf{volume zero}
		if for each $\varepsilon > 0$ exists a finite number of
		boxes $R_1,\dots,R_k$ such that:
		\[
			\sum_{i=1}^{k}{\mathrm{Vol}(R_i)} < \varepsilon
			\quad \text{and} \quad
			A \subset \bigcup_{i=1}^{k}{\text{int}(R_i)}
		\]
	\end{definition}
	We see that any point $x \in \R^n$ is of volume zero because
	we can always choose one box of arbitrarily small volume such that
	$x$ is inside it. It is also trivial to prove that a finite
	union of sets of volume zero is of volumes zero.
	\begin{theorem}
		Let $f \colon R \to \R^n$ be bounded and suppose:
		\[
			X = \{x \in R \mid \text{$f$ is not continuous in $x$}\}
		\]
		be a set of volume $0$. Then $f$ is integrable over $R$.
	\end{theorem}
	\begin{proof}
		We know that $f$ is bounded so we can denote its bound $M$,
		and also we know that for any $\varepsilon > 0 $ exist
		$R_1,\dots,R_k$ such that:
		\begin{align*}
			\sum_{i=1}^{k}{\mathrm{Vol}(R_i)} < \frac{\varepsilon}{4M}
			\quad \text{and} \quad
			X \subset \bigcup_{i=1}^{k}{\text{int}(R_i)}
		\end{align*}
		Thus we can choose a partition $P_X$ such that for every $i$
		we have $R_i \in P$ and then we have:
		\[
			U(f,P) - L(f,P) = 
			\sum_{i=1}^{k}{(M_Q-m_Q)\mathrm{Vol}(R_i)} < 
			\frac{\varepsilon}{2}
			\quad \text{and} \quad
			X \subset \bigcup_{i=1}^{k}{\text{int}(R_i)}
		\]
		Now if we consider the set $Y = R \setminus \{R_i\}_i$ then
		it is clearly closed and bounded and thus it is integrable
		and we can choose a partition $P_Y$ of $Y$ such that:
		\[
			U(f,P_Y) - L(f,P_Y) = 
			\sum_{Q \sim P_Y}{(M_Q-m_Q)\mathrm{Vol}(Q)} < 
			\frac{\varepsilon}{2}
		\]
		And now it is clear that the partition $P = P_X \cup P_Y$
		satisfies:
		\[
			U(f,P) - L(f,P) < \varepsilon
		\]
	\end{proof}
	\begin{definition}
		We say that $A \subset \R^n$ is of \textbf{measure zero} if
		exist $\{R_i\}_{i \in I}$ such that $|I| < \aleph_0$ and:
		\[
			\sum_{i \in I}{\mathrm{Vol}(R_i)} < \varepsilon
			\quad \text{and} \quad
			A \subset \bigcup_{i \in I}{\text{int}(R_i)}
		\]
	\end{definition}
	\begin{remark}
		A set of volume zero is always of measure zero, but the
		contrary is not always true. For example the sets of the
		form $\Q^n \subset \R^n$ are all of measure zero but not
		of volume zero.
	\end{remark}
	\begin{theorem}
		A compact set $D \subset \R^n$ is of volume zero if and only
		if it is of measure zero.
	\end{theorem}
	The proof of this theorem is very direct and thus omitted.
	\begin{theorem}
		\tt{Lebesgue's Theorem} Let $R \subset \R^n$ be 
		a box and let $f \colon R \to \R^n$ be bounded. Then $f$ is integrable
		if and only if the set of discontinuity points of $f$ is of 
		meausre zero.
	\end{theorem}
	\noindent
	Before proving the theorem we will define the \textbf{oscillation}
	of $f$ over a set $B \subset \R^n$ as:
	\[
		\omega(f,B) = 
		\sup\{f(x) \mid x \in R \cap B\} -
		\inf\{f(x) \mid x \in R \cap B\}
	\]
	And also define the \textbf{oscillation} of $f$ at a point 
	$x \in R$ as:
	\[
		\omega(f,x) = \lim_{r \to 0^+}{\omega(f,U_r(x))}
	\]
	Where $U_r(x) = (x_1-r,x_1+r) \times \cdots \times (x_n-r,x_n+r)$
	which is the open ball with radius $r$ around $x$ with the
	infinity norm.
	\begin{lemma}
		The function $f$ is continuous at $x \in R$ if and only of
		$\omega(f,x) = 0$.
	\end{lemma}
	\begin{lemma}
		For every $\varepsilon > 0$ the set:
		\[
			W_\varepsilon = 
			\{x \in R \mid \omega(f,x) \geq \varepsilon\}
		\]
		Is continuous.
	\end{lemma}
	We also define another set:
	\[
		W = \{x \in R \mid \omega(f,x) \neq 0\}		
	\]
	\begin{proof}
		\todo[inline]{MISSING}
	\end{proof}
	
	\begin{definition}
		A space $\Omega \subset \R^n$ is said to \textbf{have volume}
		if it is bounded and $\partial \Omega$ is of measure zero.
	\end{definition}
	\begin{definition}
		Let $\Omega \subset \R^n$ be a space with volume, let
		$f \colon R \to \R^n$ be a function, and assume that
		$\Omega \subset R$. We say that $f$ is 
		\textbf{integrable over $\Omega$} if $f\cdot1_\Omega$
		is integrable over $R$ and:
		\[
			\int_{\Omega}{f\,dV} = \int_{R}{f\cdot1_\Omega\,dV}
		\]
	\end{definition}
	\begin{theorem}
		If $f \colon \Omega \to \R^n$ is continuous and $\Omega$
		has volume then $f$ is integrable over $\Omega$.
	\end{theorem}
	\begin{definition}
		The volume of a set that has volume $A \subset \R^n$ is 
		defined as:
		\[
			\mathrm{Vol}(A) = \int_{A}{1\,dv}
		\]
	\end{definition}
	We notice that this function is well defined because the constant
	function $f \equiv 1$ is always continuous on any domain that
	has volume.
	\begin{remark}
		A Riemann integral on a space with volume has all the basic
		properties as we defined them on intervals: linearity,
		additivity, the triangle inequlity, and more. 
	\end{remark}
	\begin{theorem}
		The graph of a Riemann integrable function is of measure 
		zero, or volume zero if the function was defined on a
		compact space. A manifold has measure zero. The image
		of a manifold by a smooth transformation is of volume zero. 
	\end{theorem}
	
	
	
	
	
	
	
	
	
	
\end{document}
