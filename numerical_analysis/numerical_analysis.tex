\documentclass[11pt,a4paper]{article}

\def\nyear {2025}
\def\nterm {Winter}
\def\nlecturer {}
\def\ncourse {Introduction to Numerical Analysis}
\def\ncoursehead {Numerical Analysis}

\input{../header.tex}

\begin{document}
\maketitle

% Insert cool image here

\newpage
\tableofcontents
\newpage

\section{Introduction}
This course addresses what of all the math we have learned so far can
we compute with the computer

For example, given the task of computing the determinant of a matrix $A$,
we want to find the most efficient algorithm.
The most efficient algorithm is the one that uses the least amount of
operations.

One way of calculating $\det A$ is using permutations:
\[
  \det A = \sum_{\gamma \in S_n}
  \sgn(\gamma) a_{1 \sigma(1)} \cdot a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
\]
Each permutation costs us $n$ operations of multiplication, and to calculate
the sum we perform $n! - 1$ summation operations.
Since there are $n!$ permutations in $S_n$, the total amount of operations
of this algorithm is $n \cdot n! + n! - 1$.

An alternative algorithm would be to find the eigendecomposition
$A = U^{T} D U$ where $U$ is an orthogonal matrix, and then calculate
the product of the elements on the main diagonal of $D$ (which are the
eigenvalues of $A$).
Using this algorithm we can compute $\det A$ in $C n^3 + n - 1$ operations
for some costant $C$.
Although this algorithm is much faster than the previous ones, there are
more efficient algorithms still.

\begin{definition}[Big $O$ notation]
  Let $(x_n)_{n \geq 1}$ and $(y_n)_{n \geq 1}$ be sequences of real numbers.
  We say that $x_n = O(y_n)$ if there exist constants $C$ and $N$ such that
  for all $n > N$
  \[
    |x_n| \le C |y_n|
  \]
\end{definition}

Here are some examples
\begin{align*}
  \frac{n + 1}{n^2} &= O\del{\frac 1n} \\
  Cn^3 + n - 1 &= O(n^3) \\
  n \cdot n! + n! - 1 &= O(n \cdot n!)
\end{align*}

Using big $O$ notation we can say that direct determinant calculations
requires $O(n \dots n!)$ calculations, while using eigendecomposition
to calculate it takes $O(n^3)$ operations.

However, the big $O$ notation is still not very satisfying because
we still have
\[
  C n^3 + n - 1 = O(n \cdot n!).
\]
To fix this problem we introduce the $\Theta$ notation.

\begin{definition}[big $\Theta$ notation]
  Let $(x_n)_{n \geq 1}$ and $(y_n)_{n \geq 1}$ be sequences of real numbers.
  We say that $x_n = \Theta(y_n)$ if there exist constants $0 < c < C$ and $N$ 
  such that for all $n > N$
  \[
    c |y_n| \le |x_n| \le C |y_n|.
  \]
\end{definition}

Here are some examples
\begin{align*}
  a_k n^k + \cdots + a_1 n + a_0 &= \Theta(n^k) \\
  Cn^3 + n - 1 &= \Theta(n^3) \\
  n \cdot n! + n! - 1 &= \Theta(n \cdot n!)
\end{align*}

Another important method before we move to the next section,
is Horner's method.
It allows computing a polynomial $p(x) = \sum_{k=0}^{n} a_k x^n$ in
$\Theta(n)$ operations instead of $\Theta(n^2)$ operations.
It states that
\[
  \begin{aligned}
    &a_{0} + a_{1}x + a_{2}x^{2} + a_{3}x^{3} + \cdots + a_{n}x^{n} \\ = {}
    &a_{0} + x\del{a_{1} + x\del{a_{2} + x\del{a_{3} + \cdots + 
    x(a_{n-1} + x a_{n}) \cdots}}}.
  \end{aligned}
\]

\section{Digital Number Representation}
We usually use base-$10$ expansion to represent number.
For example, the number $x = 123.45$ means
\[
  x = 1 \times 10^2 + 2 \times 10^1 + 3 \times 10^0 + 4 \times 10^{-1} +
    5 \times 10^{-2}.
\]
In this case, we say that $x$ has a finite base-$10$ expansion.
All non-negative real numbers have a (possibly infinite) base-$10$ expansion.

\begin{theorem}
  \label{thm:bases}
  Let $x$ be a real number in $[0,1)$, and $b \geq 2$ an integer.
  Then there exists $L \in \Z$ and a sequence $(c_k)_{k=1}^{\infty}$ where
  each $c_k$ is in $\set{0,1,\dots,b-1}$, so that
  \[
    x = \sum_{k=1}^{\infty} c_k b^{-k}.
  \]
\end{theorem}

The motivation for this theorem is to prove for example, that computers,
who work in base $2$, can represent the same numbers we can represent
in the way that is more convenient to us like base $10$.

\begin{lemma}
  \label{lem:bases}
  If $b \geq 2$ is an integer, $L \in \Z$ and $x \in [0,b^{L+1})$.
  Then there exists $c_L \in \set{0,1,\dots,b-1}$ such that
  $x - c_L b^{L} \in [0,b^L)$.
\end{lemma}
\begin{proof}
  We notice that
  \[
    \bigcup_{j=0}^{b-1} \intco{j b^L, (j + 1) b^L} =
    [0, b^{L+1}).
  \]
  So there exists $c_L \in \set{0,1,\dots,b-1}$ so that 
  $x \in [c_L b^L, (c_L + 1) b^L)$.
  Then
  \[
    r_L = x - c_L b^L \in [0,b^L).
  \]
\end{proof}

We can now go back to prove \Cref{thm:bases}

\begin{proof}
  By assumption $x \in [0,b^0)$.
  By \Cref{lem:bases} with $L = -1$ we now define:
  \[
    r_1 := x - c_1 b^{-1} \in [0,b^{-1}).
  \]
  And continue indefinitely
  \[
    r_S := x - \sum_{k=1}^{S} c_k b^{-k} \in [0,b^{-k}).
  \]
  This implies that $r_S \taking{S \to \infty} 0$ which means that
  \[
    x = \sum_{k=1}^{\infty} c_k b^{-k}.
  \]
\end{proof}

We can now conclude from the theorem that for $x > 0$, we can find $L$
large enough so that $x < b^L$ and so $x b^{-L} < 1$.
We can then write
\[
  x = b^L \sum_{k=1}^{\infty} c_k b^{-k}.
\]

It is also possible in base-$b$ for $b \geq 2$ to represent any
non-negative number as:
\[
  x = (-1)^s \times [1.f]_b \times 2^m,
\]
for some $s \in \set{0,1}$, $m \geq 0$ and $f$ an infinite sequence
of digits.

\begin{remark}
  Obsviouly we can also represent $0$ because $0 \in \set{0,1,\dots,b}$.
\end{remark}

In a computer, we represent integers using $32$ bits.
We use one bit to represent the sign of the integer,
and the other as constants to the powers of two.
In this way we can represent every integer 
$n \in [- (2^{-31} - 1), 2^{31} - 1]$ as such:
\[
  n = (-1)^{c_31} \sum_{j=0}^{30}.
\]
Where $(c_0,c_1,\dots,c_{31})$ represent the bits.
If we add two numbers and get a result larger than $2^31 - 1$,
the computer will give us the result $Inf$.

When we consider rational numbers, we write them as
\[
    x = (-1)^s \times [1.f]_2 \times 2^m.
\]
We use a single bit to encode the sign, eight bits to encode $m$ (also
known as the exponent) and the remaining $32$ bits to encode $f$ (also known
as the normalized mantissa). We call $[1.f]_2$ the mantissa.
This method is known as single precision.

In double precision (which is the standard in Matlab) we use $64$ bits
to store each number. A single bit to encode the sign, $11$ bits to encode 
the exponent and the remaining $52$ bits to encode the normalized mantissa.
In general we only focus on single precision.

For now, since

% Add discussion about what's possible to represent and rounding numbers.

Denote $\floor x$ the closest number we can represent using this method.
Then the absolute error is
\[
  \abs{\floor x - x} \le 2^m 2^{-24}
\]
and the relative error is bounded by
\[
  \frac{\abs{\floor x - x}}{|x|} \le
  \frac{2^m 2^{-24}}{[1.f]_2 \times 2^m} \le
  2^{-24}.
\]
This means that the relative error of rounding is pretty low.
In double precision because we have more bits the relative error
is bounded by $2^{-52} \tilde 2 \times 10^{-16}$.
This number is called $eps$ in Matlab.

\begin{definition}[Relative error]
  Let $f(x)$ be some approximation of $x \neq 0$, the relative error
  is defined by
  \[
    \frac{|f(x) - x|}{|x|} < \epsilon
  \]
  if and only if
  \[
    f(x)= x(1 + \delta)
  \]
  for some $\delta \in (-\epsilon, \epsilon)$.
  We can get this equlity by setting $\delta = \frac{f(x) - x}{x}$.
\end{definition}

\begin{proposition}
  Let $x_1,x_2 > 0$ and assume there exists some $f \colon \R \to \R$ such that
  there exist $\delta_1$, $\delta_2$, $\delta_3$ such that
  \[
    f(x_1) = (1 + \delta_1) x_1 \stand
    f(x_2) = (1 + \delta_2) x_2 \stand
    f(f(x_1) \cdot f(x_2)) = (1 + \delta_3)[f(x_1) \cdot f(x_2)]
  \]
  where $|\delta_1|, |\delta_2|, |\delta_3| < \epsilon$.
  Then
  \[
    f(f(x_1) \cdot f(x_2)) =
    (1 + \delta) x_1 x_2
  \]
  For some $|\delta| < 3 \epsilon + 3 \epsilon^2 + \epsilon^3$.
\end{proposition}
\begin{proof}
  \[
    f(f(x_1) \cdot f(x_2)) =
    (1 + \delta_3)[f(x_1) \cdot f(x_2)] =
    (1 + \delta_3)(1 + \delta_2)(1 + \delta_1) x_1 x_2
  \]
  and
  \[
    (1 + \delta_3)(1 + \delta_2)(1 + \delta_1) =
    1 + \delta_1 + \delta_2 + \delta_3 + 
    \delta_1 \delta_2 + \delta_1 \delta_3 + \delta_2 \delta_3 + 
    \delta_1 \delta_2 \delta_3.
  \]
  Denote
  \[
    \delta =
    \delta_1 + \delta_2 + \delta_3 + 
    \delta_1 \delta_2 + \delta_1 \delta_3 + \delta_2 \delta_3 + 
    \delta_1 \delta_2 \delta_3.
  \]
  we get
  \[
    |\delta| \le 3 \epsilon + 3 \epsilon^2 + \epsilon^3
  \]
  which completes the proof.
\end{proof}

The above theorem shows that when taking the product of two numbers,
the error is bigger than epsilon, but not by much.
The situation is similar for addition and division,
but not for subtraction.

Consider
\[
  x_1 = [1.f0]_2 \stand x_2 = [1.f6]
\]
we have that $\floor{x_1} - \floor{x_2} = 0$ which implies that
relative error is
\[
  \frac{\abs{\floor{\floor{x_1} - \floor{x_2}} - |x_1 - x_2|}}{|x_1 - x_2|} = 1.
\]
This occurs when the distance between $x_1$ and $x_2$ is very small compared
to $x_1$.

\begin{proposition}
  Assume that $x_2 > x_1 > 0$ and $\frac{x_2}{x_2 - x_1} < a$ for some $a$.
  Let $f \colon \R \to \R$ be a function satisfiying
  \[
    f(x_1) = (1 + \delta_1) x_1 \stand
    f(x_2) = (1 + \delta_2) x_2 \stand
    f(f(x_1) - f(x_2)) = (1 + \delta_3)[f(x_1) - f(x_2)]
  \]
  where $|\delta_1|, |\delta_2|, |\delta_3| < \epsilon$.
  Then
  \[
    f(f(x_1) - f(x_2)) = (1 + \delta)(x_2 - x_1)
  \]
  where $|\delta| < 2 \epsilon + 2 a \epsilon + \epsilon^2 + 2 a \epsilon^2$.
\end{proposition}
\begin{proof}
  The proof is simply by calculation
\end{proof}

Because of the propositions we have seen so far, some ways to compute
certain expressions are more accurate than others.
For example instead of computing
\[
  x - \sin(x) \tor
  \sqrt{x^2 + 1} - 1
\]
to get a more accurate calculation we can calculate the equivalent
expressions
\[
  - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} \cdots
  \tand
  \frac{x^2}{\sqrt{x^2 + 1} + 1}.
\]

As mentioned earlier, in single precision we can not represent numbers bigger
than $2^{128}$ or smaller than $2^{-128}$.
Instead the computer will return us the values $\infty$ and $\ninfty$ 
respectively. This is called \emph{overflow}.
We also can not represent numbers in the range $(-2^{-128}, 2^{-128})$.
Instead the computer will return us the values $0$ or $-0$ if the number
was negative. This is called \emph{underflow}.

\begin{definition}[Softmax function]
  For a fixed $\alpha \geq 0$, the softmax function 
  $s^{\alpha} \colon \R^d \to \R^d$ is defined as
  \[
    s^{\alpha}_i := \frac{e^{\alpha x_i}}{\sum_{j=1}^{d} e^{\alpha} x_j}.
  \]
\end{definition}
\begin{remark}
  For all $x \in \R^d$ and $\alpha \geq 0$ the entries of $s^{\alpha}(x)$
  are all positive and sum to one.
\end{remark}
\begin{remark}
  If $k$ is some index such that $x_k > x_i$ for all $i \neq k$, then
  \[
    \lim_{\alpha \infty} s6{\alpha}(x) = e_k
  \]
  where $e_k$ is the unit vector with $1$ in the $k$th index and $0$ otherwise.
  Thus, for large $\alpha$ softmax gives a strong idication of where the
  maximum of the vector is located.
\end{remark}
\begin{remark}
  If $\alpha = 0$ then $s^0(0) = (1/d, 1/d, \dots, 1/d)$ for all $x$.
\end{remark}

This function is intresting because although the entries in the result vectors
are all between $0$ and $1$, the exponentiation can lead to underflow and
overflow which will greatly affect its computation.

To avoid overflow when computing softmax we can notice that for any scalar
$M$ and vector $1_d = (1, 1, \dots, 1)$ we have that 
$s^{\alpha}(x) = s^{\alpha}(x - M 1_d)$.
This allows use to choose $M = \max\set{x_i}_i$ and compute
$s^{\alpha}(x - M 1_d) = s^{\alpha}(x)$ without overflow.

\subsection{Alternatives for symbolic computation*}

The method we have used so far to calculate nubmbers is called numercial
computation.
Because of the way we represent numbers in (floating points) we always
get an error bounded by $2^{-24}$.
There are some alternatives to numerical calculation.

One alternative for numerical calculations is \emph{symbolic calculations}.
This method is used by Mathematica, Maple, Wolfram Alpha 
and can also be used in Matlab.
It has two main behaviours
\begin{itemize}
  \item When multiplying numbers, the numbers of digits increase to avoid
    truncation.
  \item Solutions of equations can be given symbolically like $\sqrt{2}$,
    $\pi$ etc.
\end{itemize}
Using this method we gain more accuracy, but lose speed of computation.

Another alternative is called \emph{interval arithmetics}.




\end{document}
