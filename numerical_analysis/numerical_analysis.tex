\documentclass[11pt,a4paper]{article}

\def\nyear {2025}
\def\nterm {Winter}
\def\nlecturer {}
\def\ncourse {Introduction to Numerical Analysis}
\def\ncoursehead {Numerical Analysis}

\input{../header.tex}

\begin{document}
\maketitle

% Insert cool image here

\newpage
\tableofcontents
\newpage

\section{Introduction}
This course addresses what of all the math we have learned so far can
we compute with the computer

For example, given the task of computing the determinant of a matrix $A$,
we want to find the most efficient algorithm.
The most efficient algorithm is the one that uses the least amount of
operations.

One way of calculating $\det A$ is using permutations:
\[
  \det A = \sum_{\gamma \in S_n}
  \sgn(\gamma) a_{1 \sigma(1)} \cdot a_{2 \sigma(2)} \cdots a_{n \sigma(n)}.
\]
Each permutation costs us $n$ operations of multiplication, and to calculate
the sum we perform $n! - 1$ summation operations.
Since there are $n!$ permutations in $S_n$, the total amount of operations
of this algorithm is $n \cdot n! + n! - 1$.

An alternative algorithm would be to find the eigendecomposition
$A = U^{T} D U$ where $U$ is an orthogonal matrix, and then calculate
the product of the elements on the main diagonal of $D$ (which are the
eigenvalues of $A$).
Using this algorithm we can compute $\det A$ in $C n^3 + n - 1$ operations
for some costant $C$.
Although this algorithm is much faster than the previous ones, there are
more efficient algorithms still.

\begin{definition}[Big $O$ notation]
  Let $(x_n)_{n \geq 1}$ and $(y_n)_{n \geq 1}$ be sequences of real numbers.
  We say that $x_n = O(y_n)$ if there exist constants $C$ and $N$ such that
  for all $n > N$
  \[
    |x_n| \le C |y_n|
  \]
\end{definition}

Here are some examples
\begin{align*}
  \frac{n + 1}{n^2} &= O\del{\frac 1n} \\
  Cn^3 + n - 1 &= O(n^3) \\
  n \cdot n! + n! - 1 &= O(n \cdot n!)
\end{align*}

Using big $O$ notation we can say that direct determinant calculations
requires $O(n \dots n!)$ calculations, while using eigendecomposition
to calculate it takes $O(n^3)$ operations.

However, the big $O$ notation is still not very satisfying because
we still have
\[
  C n^3 + n - 1 = O(n \cdot n!).
\]
To fix this problem we introduce the $\Theta$ notation.

\begin{definition}[big $\Theta$ notation]
  Let $(x_n)_{n \geq 1}$ and $(y_n)_{n \geq 1}$ be sequences of real numbers.
  We say that $x_n = \Theta(y_n)$ if there exist constants $0 < c < C$ and $N$ 
  such that for all $n > N$
  \[
    c |y_n| \le |x_n| \le C |y_n|.
  \]
\end{definition}

Here are some examples
\begin{align*}
  a_k n^k + \cdots + a_1 n + a_0 &= \Theta(n^k) \\
  Cn^3 + n - 1 &= \Theta(n^3) \\
  n \cdot n! + n! - 1 &= \Theta(n \cdot n!)
\end{align*}

Another important method before we move to the next section,
is Horner's method.
It allows computing a polynomial $p(x) = \sum_{k=0}^{n} a_k x^n$ in
$\Theta(n)$ operations instead of $\Theta(n^2)$ operations.
It states that
\[
  \begin{aligned}
    &a_{0} + a_{1}x + a_{2}x^{2} + a_{3}x^{3} + \cdots + a_{n}x^{n} \\ = {}
    &a_{0} + x\del{a_{1} + x\del{a_{2} + x\del{a_{3} + \cdots + 
    x(a_{n-1} + x a_{n}) \cdots}}}.
  \end{aligned}
\]

\section{Digital Number Representation}
We usually use base-$10$ expansion to represent number.
For example, the number $x = 123.45$ means
\[
  x = 1 \times 10^2 + 2 \times 10^1 + 3 \times 10^0 + 4 \times 10^{-1} +
    5 \times 10^{-2}.
\]
In this case, we say that $x$ has a finite base-$10$ expansion.
All non-negative real numbers have a (possibly infinite) base-$10$ expansion.

\begin{theorem}
  \label{thm:bases}
  Let $x$ be a real number in $[0,1)$, and $b \geq 2$ an integer.
  Then there exists $L \in \Z$ and a sequence $(c_k)_{k=1}^{\infty}$ where
  each $c_k$ is in $\set{0,1,\dots,b-1}$, so that
  \[
    x = \sum_{k=1}^{\infty} c_k b^{-k}.
  \]
\end{theorem}

The motivation for this theorem is to prove for example, that computers,
who work in base $2$, can represent the same numbers we can represent
in the way that is more convenient to us like base $10$.

\begin{lemma}
  \label{lem:bases}
  If $b \geq 2$ is an integer, $L \in \Z$ and $x \in [0,b^{L+1})$.
  Then there exists $c_L \in \set{0,1,\dots,b-1}$ such that
  $x - c_L b^{L} \in [0,b^L)$.
\end{lemma}
\begin{proof}
  We notice that
  \[
    \bigcup_{j=0}^{b-1} \intco{j b^L, (j + 1) b^L} =
    [0, b^{L+1}).
  \]
  So there exists $c_L \in \set{0,1,\dots,b-1}$ so that 
  $x \in [c_L b^L, (c_L + 1) b^L)$.
  Then
  \[
    r_L = x - c_L b^L \in [0,b^L).
  \]
\end{proof}

We can now go back to prove \Cref{thm:bases}

\begin{proof}
  By assumption $x \in [0,b^0)$.
  By \Cref{lem:bases} with $L = -1$ we now define:
  \[
    r_1 := x - c_1 b^{-1} \in [0,b^{-1}).
  \]
  And continue indefinitely
  \[
    r_S := x - \sum_{k=1}^{S} c_k b^{-k} \in [0,b^{-k}).
  \]
  This implies that $r_S \taking{S \to \infty} 0$ which means that
  \[
    x = \sum_{k=1}^{\infty} c_k b^{-k}.
  \]
\end{proof}

We can now conclude from the theorem that for $x > 0$, we can find $L$
large enough so that $x < b^L$ and so $x b^{-L} < 1$.
We can then write
\[
  x = b^L \sum_{k=1}^{\infty} c_k b^{-k}.
\]

It is also possible in base-$b$ for $b \geq 2$ to represent any
non-negative number as:
\[
  x = (-1)^s \times [1.f]_b \times 2^m,
\]
for some $s \in \set{0,1}$, $m \geq 0$ and $f$ an infinite sequence
of digits.

\begin{remark}
  Obsviouly we can also represent $0$ because $0 \in \set{0,1,\dots,b}$.
\end{remark}

In a computer, we represent integers using $32$ bits.
We use one bit to represent the sign of the integer,
and the other as constants to the powers of two.
In this way we can represent every integer 
$n \in [- (2^{-31} - 1), 2^{31} - 1]$ as such:
\[
  n = (-1)^{c_31} \sum_{j=0}^{30}.
\]
Where $(c_0,c_1,\dots,c_{31})$ represent the bits.
If we add two numbers and get a result larger than $2^31 - 1$,
the computer will give us the result $Inf$.

When we consider rational numbers, we write them as
\[
    x = (-1)^s \times [1.f]_2 \times 2^m.
\]
We use a single bit to encode the sign, eight bits to encode $m$ (also
known as the exponent) and the remaining $32$ bits to encode $f$ (also known
as the normalized mantissa). We call $[1.f]_2$ the mantissa.
This method is known as single precision.

In double precision (which is the standard in Matlab) we use $64$ bits
to store each number. A single bit to encode the sign, $11$ bits to encode 
the exponent and the remaining $52$ bits to encode the normalized mantissa.
In general we only focus on single precision.

For now, since

% Add discussion about what's possible to represent and rounding numbers.

Denote $\floor x$ the closest number we can represent using this method.
Then the absolute error is
\[
  \abs{\floor x - x} \le 2^m 2^{-24}
\]
and the relative error is bounded by
\[
  \frac{\abs{\floor x - x}}{|x|} \le
  \frac{2^m 2^{-24}}{[1.f]_2 \times 2^m} \le
  2^{-24}.
\]
This means that the relative error of rounding is pretty low.
In double precision because we have more bits the relative error
is bounded by $2^{-52} \tilde 2 \times 10^{-16}$.
This number is called $eps$ in Matlab.

\begin{definition}[Relative error]
  Let $f(x)$ be some approximation of $x \neq 0$, the relative error
  is defined by
  \[
    \frac{|f(x) - x|}{|x|} < \epsilon
  \]
  if and only if
  \[
    f(x)= x(1 + \delta)
  \]
  for some $\delta \in (-\epsilon, \epsilon)$.
  We can get this equlity by setting $\delta = \frac{f(x) - x}{x}$.
\end{definition}

\begin{proposition}
  Let $x_1,x_2 > 0$ and assume there exists some $f \colon \R \to \R$ such that
  there exist $\delta_1$, $\delta_2$, $\delta_3$ such that
  \[
    f(x_1) = (1 + \delta_1) x_1 \stand
    f(x_2) = (1 + \delta_2) x_2 \stand
    f(f(x_1) \cdot f(x_2)) = (1 + \delta_3)[f(x_1) \cdot f(x_2)]
  \]
  where $|\delta_1|, |\delta_2|, |\delta_3| < \epsilon$.
  Then
  \[
    f(f(x_1) \cdot f(x_2)) =
    (1 + \delta) x_1 x_2
  \]
  For some $|\delta| < 3 \epsilon + 3 \epsilon^2 + \epsilon^3$.
\end{proposition}
\begin{proof}
  \[
    f(f(x_1) \cdot f(x_2)) =
    (1 + \delta_3)[f(x_1) \cdot f(x_2)] =
    (1 + \delta_3)(1 + \delta_2)(1 + \delta_1) x_1 x_2
  \]
  and
  \[
    (1 + \delta_3)(1 + \delta_2)(1 + \delta_1) =
    1 + \delta_1 + \delta_2 + \delta_3 + 
    \delta_1 \delta_2 + \delta_1 \delta_3 + \delta_2 \delta_3 + 
    \delta_1 \delta_2 \delta_3.
  \]
  Denote
  \[
    \delta =
    \delta_1 + \delta_2 + \delta_3 + 
    \delta_1 \delta_2 + \delta_1 \delta_3 + \delta_2 \delta_3 + 
    \delta_1 \delta_2 \delta_3.
  \]
  we get
  \[
    |\delta| \le 3 \epsilon + 3 \epsilon^2 + \epsilon^3
  \]
  which completes the proof.
\end{proof}

The above theorem shows that when taking the product of two numbers,
the error is bigger than epsilon, but not by much.
The situation is similar for addition and division,
but not for subtraction.

Consider
\[
  x_1 = [1.f0]_2 \stand x_2 = [1.f6]
\]
we have that $\floor{x_1} - \floor{x_2} = 0$ which implies that
relative error is
\[
  \frac{\abs{\floor{\floor{x_1} - \floor{x_2}} - |x_1 - x_2|}}{|x_1 - x_2|} = 1.
\]
This occurs when the distance between $x_1$ and $x_2$ is very small compared
to $x_1$.

\begin{proposition}
  Assume that $x_2 > x_1 > 0$ and $\frac{x_2}{x_2 - x_1} < a$ for some $a$.
  Let $f \colon \R \to \R$ be a function satisfiying
  \[
    f(x_1) = (1 + \delta_1) x_1 \stand
    f(x_2) = (1 + \delta_2) x_2 \stand
    f(f(x_1) - f(x_2)) = (1 + \delta_3)[f(x_1) - f(x_2)]
  \]
  where $|\delta_1|, |\delta_2|, |\delta_3| < \epsilon$.
  Then
  \[
    f(f(x_1) - f(x_2)) = (1 + \delta)(x_2 - x_1)
  \]
  where $|\delta| < 2 \epsilon + 2 a \epsilon + \epsilon^2 + 2 a \epsilon^2$.
\end{proposition}
\begin{proof}
  The proof is simply by calculation
\end{proof}

Because of the propositions we have seen so far, some ways to compute
certain expressions are more accurate than others.
For example instead of computing
\[
  x - \sin(x) \tor
  \sqrt{x^2 + 1} - 1
\]
to get a more accurate calculation we can calculate the equivalent
expressions
\[
  - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} \cdots
  \tand
  \frac{x^2}{\sqrt{x^2 + 1} + 1}.
\]

As mentioned earlier, in single precision we can not represent numbers bigger
than $2^{128}$ or smaller than $2^{-128}$.
Instead the computer will return us the values $\infty$ and $\ninfty$ 
respectively. This is called \emph{overflow}.
We also can not represent numbers in the range $(-2^{-128}, 2^{-128})$.
Instead the computer will return us the values $0$ or $-0$ if the number
was negative. This is called \emph{underflow}.

\begin{definition}[Softmax function]
  For a fixed $\alpha \geq 0$, the softmax function 
  $s^{\alpha} \colon \R^d \to \R^d$ is defined as
  \[
    s^{\alpha}_i := \frac{e^{\alpha x_i}}{\sum_{j=1}^{d} e^{\alpha} x_j}.
  \]
\end{definition}
\begin{remark}
  For all $x \in \R^d$ and $\alpha \geq 0$ the entries of $s^{\alpha}(x)$
  are all positive and sum to one.
\end{remark}
\begin{remark}
  If $k$ is some index such that $x_k > x_i$ for all $i \neq k$, then
  \[
    \lim_{\alpha \infty} s6{\alpha}(x) = e_k
  \]
  where $e_k$ is the unit vector with $1$ in the $k$th index and $0$ otherwise.
  Thus, for large $\alpha$ softmax gives a strong idication of where the
  maximum of the vector is located.
\end{remark}
\begin{remark}
  If $\alpha = 0$ then $s^0(0) = (1/d, 1/d, \dots, 1/d)$ for all $x$.
\end{remark}

This function is intresting because although the entries in the result vectors
are all between $0$ and $1$, the exponentiation can lead to underflow and
overflow which will greatly affect its computation.

To avoid overflow when computing softmax we can notice that for any scalar
$M$ and vector $1_d = (1, 1, \dots, 1)$ we have that 
$s^{\alpha}(x) = s^{\alpha}(x - M 1_d)$.
This allows use to choose $M = \max\set{x_i}_i$ and compute
$s^{\alpha}(x - M 1_d) = s^{\alpha}(x)$ without overflow.

\subsection{Alternatives for symbolic computation*}

The method we have used so far to calculate nubmbers is called numercial
computation.
Because of the way we represent numbers in (floating points) we always
get an error bounded by $2^{-24}$.
There are some alternatives to numerical calculation.

One alternative for numerical calculations is \emph{symbolic calculations}.
This method is used by Mathematica, Maple, Wolfram Alpha 
and can also be used in Matlab.
It has two main behaviours
\begin{itemize}
  \item When multiplying numbers, the numbers of digits increase to avoid
    truncation.
  \item Solutions of equations can be given symbolically like $\sqrt{2}$,
    $\pi$ etc.
\end{itemize}
Using this method we gain more accuracy, but lose speed of computation.

Another alternative is called \emph{interval arithmetics}.
Since our calculations are not exact we can represent numbers using intevals.
For example, let $x = [1.f]_2$ such that $f = f_1 \circ f_2$.
where $f_1$ is the first 23 digits of $f$ and $f_2$ is the rest of the digits.
In this case, we use the interval $I_x = [[1.f_1]_2,[1.f_1 1]_2]$ to represent
$x$ because $x \in I_X$.

Suppose we have two numbers $x = [a_x, b_x]$ and $y = [a_y, b_y]$, when
calculating the sum $x + y$ we get $x + y = [a_x + a_y, b_x + b_y]$.
In this way we can be certain that
$x + y \in I_{x+y} = [a_x + a_y, b_x + b_y]$.
We define multiplication in a similar, though more complicated way.
When tying to define multiplication for intercal arithmetics, one could
use the following proposition.

\begin{proposition}
  The minimum and maximum of the function $f(x, y) = xy$ over
  the rectangle $[a_x, b_x] \times [a_y, b_y]$ are obtained at the corners. 
\end{proposition}

\section{Solving non-linear equations}
From now on, we assume we can store numbers without inaccuracies, and
perfrom arithmetics without errors.

\subsection{Bisection method}
Suppose we wanted to find a solution to the equation
\[
  \sin(x) = e^x.
\]
Then from analysis courses we know to approximate the solution by changing
the equaition to standard form
\[
  f(x) = e^x - \sin(x)
\]
and then find an interval like $[a, b] = [-\frac{3 \pi}{2}, 1]$ for which
$f(a) < 0 < f(b)$.
Because then we know that there exists a solution $x \in [a,b]$.

The bisection method allows us to find roots of continuous functions in
intervals $[a,b]$ such that $\mathrm{sign} f(a) \neq \mathrm{sign} f(b)$.
It works by repeating the following process
\begin{enumerate}
  \item[(-1)] Set $a_0 = a$, $b_0 = b$ and $c_0 = \frac{a_0 + b_0}{2}$.
  \item[(n)] Assume $a_n$, $b_n$, $c_n$ are defined. If $f(c_n) = 0$
    we have found a root. Otherwise, from the pigeonhole principle there
    exists $x \in \set{a_n,b_n}$ such that $f(c_n)$ and $f(x)$ form an interval
    that contains $0$. We call this interval $[a_{n + 1}, b_{n + 1}]$.
    We set $c_{n + 1} = \frac{a_{n + 1} + b_{n + 1}}{2}$. Treating $n$
    as a variable for formality, we increment it by one, and continue to
    the next step (step $(n)$).
\end{enumerate}
Setting $|a - b| = L$ we get that in the $|a_n - b_n| = L 2^{-n}$.
By Cantor's intersection theorem, and the squeeze theorem, it is clear
that $c_n \taking{n \to \infty} x$ where $x$ is a root of $f(x)$.

\begin{remark}
  Using the bisection method, we can calculate the number of steps we need to
  take to obtain $x'$ where $x' \in [x - \epsilon, x + \epsilon]$ for any
  $\epsilon > 0$.
\end{remark}

% Maybe generate psuedpcode for the bisection method using chatGPT.

\subsection{Newton's algorithm}
Newton's algorithm (also known as the Newton--Raphson algorithm) is an algorithm
to compute the root of a function $f \colon \R \to \R$.
We will assume $f \in C^2(\R)$ and that $f$ has a root $x$.

The idea behind the algorithm is to have a first guess of the root $x_0$,
and then set $x_{n+1}$ to be the root of the Taylor expansion up to linear
order of $f$ at the point $x_n$.

This algorithm does not always converge to the root,
% example
but if we have a good approximation for the root as $x_0$ it will always
converge to the root very quickly.

\begin{example}
  Consider the function $f(x) = e^x - 6$.
  The solution is $x = \log 6 \approx 1.7918$.
  We start by a close guess $x_0 = 2$.
  The iterations are given by
  \[
    x_{n + 1} =
    x_n - \frac{f(x_n)}{f'(x_n)} =
    x_n - \frac{e^{x_n} - 6}{e^{x_n}}.
  \]
  Applying this process gives
  \[
    x_1 = 1.8120 \stand
    x_2 = 1.7920 \stand
    x_3 = 1.7918 \stand
    x_4 = 1.7918
  \]
  with errors
  \[
    e_1 = 2 \times 10^{-2} \stand
    e_2 = 2 \times 10^{-4} \stand
    e_3 = 2 \times 10^{-8} \stand
    e_4 = 4 \times 10^{-16}.
  \]
  which shows that the process is very useful.
\end{example}

\begin{lemma}
  \label{lem:newton-local}
  Let $f \in C^2(\R)$ be a function with a root $r \in \R$.
  Let $x_{n + 1}$ be the solution of $L(x \mid x_n) = 0$ which is
  the root of the linear Taylor approximation of $f(x)$ at $x_n$.
  Then there exists $\xi \in (r,x_n)$ such that
  \[
    f'(x_n)(x_{n+1} - r) = \half f''(\xi)(x_n - r)^2
  \]
\end{lemma}
\begin{proof}
  By definition we have $L(x_{n + 1} \mid x_n) = 0$, and Taylor's expansion
  around $x_n$ gives us
  \[
    0 = f(r) = L(r \mid x_n) + \half f''(\xi)(x_n - r)^2
  \]
  so
  \[
    f'(x_n)(x_{n+1} - r) = L(x_{n + 1} \mid x_n) - L(r \mid x_n) =
    L(r \mid x_n) = \half f''(\xi)(x_n - r)^2
  \]
  which completes the proof.
\end{proof}

\begin{proposition}
  Let $f \in C^2(\R)$ be a function with a simple root $r \in \R$.
  Then there exists $\delta > 0$ and $C > 0$ such that for every Newton
  iterations starting from $x_0 \in [r - \delta, r + \delta]$, the
  sequence $|x_n - r|$ converges monotonely to zero and
  \[
    |x_{n  + 1} - r| \le C(x_n - r)^2
  \]
\end{proposition}
\begin{remark}
  A simple root means that $f'(r) \neq 0$.
\end{remark}
\begin{proof}
  Let $x_n$ be the sequence Newton--Raphson iterates starting from $x_0$.
  Since $f'(r) \neq 0$, we can choose some $\delta_0 > 0$ small enough so
  that $f'(x) \neq 0$ on $U = [r - \delta_0, r + \delta_0]$.
  Set
  \[
    m = \min_{x \in U} |f'(x)| \stand M = \max_{x \in U} |f''(x)|
  \]
  then using \Cref{lem:newton-local} we have that whenever 
  TO BE CONTINUED
\end{proof}

The convergence rate of Newton's method near a minimun is qaudratic.
This means that
\[
  |x_{n + 1} - r| \le C (x_n - r)^2.
\]
In comparison, the convergence rate of the bisection method was linear
\[
  |x_{n + 1} - r| \le \half |x_n - r|.
\]

\begin{definition}[Convergence rate]
  Let $x_n$ be a sequence converging to $L$.
  We say that $x_n$ has a convergence rate $\alpha > 0$ if there exists
  $C > 0$ and $N > 0$ such that for all $n < N$ we have
  \[
    |x_{n + 1} - L| \le C |x_n - L|^{\alpha}
  \]
\end{definition}

\begin{theorem}
  Assume $f \in C^2(\R)$ is strictly increasing, strictly convex, 
  and has a root.
  Then the root is unique, and Newton iterations will converge to it from
  any starting point.
\end{theorem}
\begin{proof}
  It is clear that the root is unique.
  From strictly convex, and strictly increasing we have that
  $f''(x) > 0$ and $f'(x) > 0$ for all $x$.
  From \Cref{lem:newton-local} we see that $x_n - r > 0$ for all $n \geq 1$.
  Since $f$ is increasing it follows that $f(x_n) > f(r) = 0$.
  Since
  \[
    x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}
  \]
  we see that $r < x_{n + 1} < x_n$, so $x_n$ is monotonically decreasing and
  bounded below, thus it converges to some limit $L$.
  Since $f \in C^2(\R)$ we have that
  \[
    L = 
    \lim_{x \to \infty} x_{n + 1} =
    \lim_{n \to \infty} x_n - \frac{f(x_n)}{f'(x_n)} =
    L - \frac{f(L)}{f'(L)}
  \]
  which means that $f(L) = 0$ and thus $L = r$ which completes the proof.
\end{proof}

We can also generalize Newton's method to higher dimensions.

Let $f \colon \R^d \to \R^d$ be a function such that $f \in C^2{\R^d}$
and $f$ has a root $r \in \R^d$.
Given a point $x_n \in \R^d$, we can define $x_{n + 1}$ as the solution
to the linear approximation of $f$ around $x_n$,
\[
  L(x \mid x_n) = f(x_n) + D_f(x_n)(x - x_n).
\]
The solution for the equation $L(x \mid x_n) = 0$ is
\[
  x_{n + 1} = x_n- D_f^{-1}(x_N) f(x_n)
\]













\end{document}
